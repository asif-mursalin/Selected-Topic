{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Data loading complete!\n",
      "\n",
      "=== Training global_avg model ===\n",
      "Global average rating: 3.5417\n",
      "Training time: 0.05 seconds\n",
      "\n",
      "=== Training user_item_bias model ===\n",
      "Global average: 3.5417\n",
      "User bias range: [-3.0357, 1.4582]\n",
      "Item bias range: [-3.0042, 1.3888]\n",
      "Training time: 1583.19 seconds\n",
      "\n",
      "=== Training item_cf model ===\n",
      "Calculating item-item similarity matrix...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# --------- 1. UTILITY FUNCTIONS ---------\n",
    "\n",
    "def load_processed_data(data_dir=\"processed_data\"):\n",
    "    \"\"\"\n",
    "    Load the preprocessed data created in Phase 1\n",
    "    \"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    \n",
    "    # Load ratings data splits\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train_ratings.csv'))\n",
    "    val = pd.read_csv(os.path.join(data_dir, 'val_ratings.csv'))\n",
    "    test = pd.read_csv(os.path.join(data_dir, 'test_ratings.csv'))\n",
    "    \n",
    "    # Load feature data\n",
    "    movies = pd.read_csv(os.path.join(data_dir, 'movies_processed.csv'))\n",
    "    user_features = pd.read_csv(os.path.join(data_dir, 'user_features.csv'))\n",
    "    item_features = pd.read_csv(os.path.join(data_dir, 'item_features.csv'))\n",
    "    \n",
    "    # Load ID mappings\n",
    "    user_map_df = pd.read_csv(os.path.join(data_dir, 'user_id_map.csv'))\n",
    "    movie_map_df = pd.read_csv(os.path.join(data_dir, 'movie_id_map.csv'))\n",
    "    \n",
    "    # Convert mappings to dictionaries\n",
    "    user_map = dict(zip(user_map_df['userId'], user_map_df['matrix_idx']))\n",
    "    movie_map = dict(zip(movie_map_df['movieId'], movie_map_df['matrix_idx']))\n",
    "    \n",
    "    # Reverse mappings (matrix index to ID)\n",
    "    user_map_rev = dict(zip(user_map_df['matrix_idx'], user_map_df['userId']))\n",
    "    movie_map_rev = dict(zip(movie_map_df['matrix_idx'], movie_map_df['movieId']))\n",
    "    \n",
    "    # Load sparse matrices\n",
    "    sparse_dir = os.path.join(data_dir, 'sparse_matrices')\n",
    "    train_matrix = sparse.load_npz(os.path.join(sparse_dir, 'train_matrix.npz'))\n",
    "    val_matrix = sparse.load_npz(os.path.join(sparse_dir, 'val_matrix.npz'))\n",
    "    test_matrix = sparse.load_npz(os.path.join(sparse_dir, 'test_matrix.npz'))\n",
    "    \n",
    "    print(\"Data loading complete!\")\n",
    "    \n",
    "    return {\n",
    "        'train': train,\n",
    "        'val': val,\n",
    "        'test': test,\n",
    "        'movies': movies,\n",
    "        'user_features': user_features,\n",
    "        'item_features': item_features,\n",
    "        'user_map': user_map,\n",
    "        'movie_map': movie_map,\n",
    "        'user_map_rev': user_map_rev,\n",
    "        'movie_map_rev': movie_map_rev,\n",
    "        'train_matrix': train_matrix,\n",
    "        'val_matrix': val_matrix,\n",
    "        'test_matrix': test_matrix\n",
    "    }\n",
    "\n",
    "def evaluate_model(predictions, true_ratings, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for a recommendation model\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted ratings\n",
    "        true_ratings: Ground truth ratings\n",
    "        name: Model name for display purposes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, predictions))\n",
    "    mae = np.mean(np.abs(true_ratings - predictions))\n",
    "    \n",
    "    # Calculate accuracy within different thresholds\n",
    "    within_1 = np.mean(np.abs(true_ratings - predictions) <= 1.0)\n",
    "    within_05 = np.mean(np.abs(true_ratings - predictions) <= 0.5)\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Within 0.5 stars: {within_05:.2%}\")\n",
    "    print(f\"Within 1.0 stars: {within_1:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'within_05': within_05,\n",
    "        'within_1': within_1\n",
    "    }\n",
    "\n",
    "def get_user_rated_items(user_idx, train_matrix):\n",
    "    \"\"\"Get the items rated by a specific user in the training set\"\"\"\n",
    "    user_ratings = train_matrix[user_idx].toarray().flatten()\n",
    "    return np.where(user_ratings > 0)[0]\n",
    "\n",
    "def get_top_n_recommendations(predictions, user_idx, n=10, exclude_rated=True, train_matrix=None):\n",
    "    \"\"\"\n",
    "    Get top N movie recommendations for a user based on predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions: Matrix of predicted ratings\n",
    "        user_idx: User index in the matrix\n",
    "        n: Number of recommendations to return\n",
    "        exclude_rated: Whether to exclude items the user has already rated\n",
    "        train_matrix: Training matrix (needed if exclude_rated is True)\n",
    "    \n",
    "    Returns:\n",
    "        List of top N movie indices\n",
    "    \"\"\"\n",
    "    user_pred = predictions[user_idx].copy()\n",
    "    \n",
    "    if exclude_rated and train_matrix is not None:\n",
    "        # Get items the user has already rated and set their predictions to -inf\n",
    "        rated_items = get_user_rated_items(user_idx, train_matrix)\n",
    "        user_pred[rated_items] = -np.inf\n",
    "    \n",
    "    # Get the indices of top N predictions\n",
    "    top_indices = np.argsort(user_pred)[::-1][:n]\n",
    "    \n",
    "    return top_indices\n",
    "\n",
    "# --------- 2. BASELINE MODELS ---------\n",
    "\n",
    "class GlobalAverageModel:\n",
    "    \"\"\"Simple baseline that predicts the global average rating for all users and items\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.global_avg = None\n",
    "    \n",
    "    def fit(self, train_matrix):\n",
    "        \"\"\"Calculate the global average rating from training data\"\"\"\n",
    "        start_time = time.time()\n",
    "        # Calculate global average (ignoring zeros)\n",
    "        ratings = train_matrix.data\n",
    "        self.global_avg = ratings.mean()\n",
    "        print(f\"Global average rating: {self.global_avg:.4f}\")\n",
    "        print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_indices, item_indices):\n",
    "        \"\"\"Predict using the global average for all user-item pairs\"\"\"\n",
    "        return np.full(len(user_indices), self.global_avg)\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\"Return the global average prediction for all possible user-item pairs\"\"\"\n",
    "        return self.global_avg\n",
    "\n",
    "class UserItemBiasModel:\n",
    "    \"\"\"Baseline model that incorporates user and item biases\"\"\"\n",
    "    \n",
    "    def __init__(self, reg_param=0.1):\n",
    "        self.global_avg = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        self.reg_param = reg_param  # Regularization parameter\n",
    "    \n",
    "    def fit(self, train_matrix):\n",
    "        \"\"\"Calculate global average, user biases, and item biases\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get the nonzero values in train_matrix\n",
    "        ratings = train_matrix.data\n",
    "        rows, cols = train_matrix.nonzero()\n",
    "        n_users, n_items = train_matrix.shape\n",
    "        \n",
    "        # Calculate global average\n",
    "        self.global_avg = ratings.mean()\n",
    "        \n",
    "        # Initialize bias arrays\n",
    "        self.user_biases = np.zeros(n_users)\n",
    "        self.item_biases = np.zeros(n_items)\n",
    "        \n",
    "        # Calculate user biases\n",
    "        for u in range(n_users):\n",
    "            user_ratings = train_matrix[u].data\n",
    "            if len(user_ratings) > 0:\n",
    "                self.user_biases[u] = user_ratings.mean() - self.global_avg\n",
    "                # Apply regularization\n",
    "                self.user_biases[u] *= len(user_ratings) / (len(user_ratings) + self.reg_param)\n",
    "        \n",
    "        # Calculate item biases\n",
    "        for i in range(n_items):\n",
    "            # Extract all ratings for item i (need to transpose matrix for efficiency)\n",
    "            item_ratings = train_matrix.T[i].data\n",
    "            if len(item_ratings) > 0:\n",
    "                self.item_biases[i] = item_ratings.mean() - self.global_avg\n",
    "                # Apply regularization\n",
    "                self.item_biases[i] *= len(item_ratings) / (len(item_ratings) + self.reg_param)\n",
    "        \n",
    "        print(f\"Global average: {self.global_avg:.4f}\")\n",
    "        print(f\"User bias range: [{self.user_biases.min():.4f}, {self.user_biases.max():.4f}]\")\n",
    "        print(f\"Item bias range: [{self.item_biases.min():.4f}, {self.item_biases.max():.4f}]\")\n",
    "        print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_indices, item_indices):\n",
    "        \"\"\"Predict ratings for given user-item pairs using bias model\"\"\"\n",
    "        predictions = np.zeros(len(user_indices))\n",
    "        \n",
    "        for i, (u, m) in enumerate(zip(user_indices, item_indices)):\n",
    "            predictions[i] = self.global_avg + self.user_biases[u] + self.item_biases[m]\n",
    "            \n",
    "        # Clip predictions to valid rating range [0.5, 5.0]\n",
    "        return np.clip(predictions, 0.5, 5.0)\n",
    "    \n",
    "    def predict_matrix(self):\n",
    "        \"\"\"Generate a full prediction matrix using the bias model\"\"\"\n",
    "        n_users, n_items = len(self.user_biases), len(self.item_biases)\n",
    "        \n",
    "        # Create meshgrid for broadcasting\n",
    "        users_grid, items_grid = np.meshgrid(np.arange(n_users), np.arange(n_items), indexing='ij')\n",
    "        \n",
    "        # Calculate predictions using broadcasting\n",
    "        predictions = self.global_avg + self.user_biases[users_grid] + self.item_biases[items_grid]\n",
    "        \n",
    "        # Clip predictions to valid rating range\n",
    "        return np.clip(predictions, 0.5, 5.0)\n",
    "\n",
    "# --------- 3. SIMILARITY-BASED COLLABORATIVE FILTERING ---------\n",
    "\n",
    "class ItemBasedCF:\n",
    "    \"\"\"Item-based collaborative filtering using cosine similarity\"\"\"\n",
    "    \n",
    "    def __init__(self, k=50):\n",
    "        self.k = k  # Number of similar items to consider\n",
    "        self.train_matrix = None\n",
    "        self.item_similarity = None\n",
    "    \n",
    "    def fit(self, train_matrix, calculate_similarity=True):\n",
    "        \"\"\"\n",
    "        Calculate item-item similarity matrix\n",
    "        \n",
    "        Args:\n",
    "            train_matrix: User-item rating matrix\n",
    "            calculate_similarity: Whether to calculate similarity matrix or use pre-computed one\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train_matrix = train_matrix\n",
    "        \n",
    "        if calculate_similarity:\n",
    "            print(\"Calculating item-item similarity matrix...\")\n",
    "            # Normalize the ratings by subtracting user means\n",
    "            user_means = np.array([rating.mean() if rating.size > 0 else 0 \n",
    "                                  for rating in [train_matrix[u].data for u in range(train_matrix.shape[0])]])\n",
    "            \n",
    "            # Create a normalized matrix for better similarity calculation\n",
    "            normalized_matrix = train_matrix.copy()\n",
    "            for u in range(train_matrix.shape[0]):\n",
    "                u_ratings = normalized_matrix[u].nonzero()[1]\n",
    "                if len(u_ratings) > 0:\n",
    "                    normalized_matrix[u, u_ratings] = train_matrix[u, u_ratings].toarray()[0] - user_means[u]\n",
    "            \n",
    "            # Calculate item similarities using cosine similarity\n",
    "            self.item_similarity = cosine_similarity(normalized_matrix.T)\n",
    "            \n",
    "            # Zero out self-similarity to avoid trivial recommendations\n",
    "            np.fill_diagonal(self.item_similarity, 0)\n",
    "            \n",
    "            print(f\"Item similarity matrix computed. Shape: {self.item_similarity.shape}\")\n",
    "            print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
    "        else:\n",
    "            print(\"Using pre-computed similarity matrix\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_idx: Item index\n",
    "            \n",
    "        Returns:\n",
    "            Predicted rating\n",
    "        \"\"\"\n",
    "        # Get items rated by the user\n",
    "        user_ratings = self.train_matrix[user_idx].toarray().flatten()\n",
    "        rated_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        if len(rated_items) == 0:\n",
    "            # If user has no ratings, return global average\n",
    "            return np.mean(self.train_matrix.data)\n",
    "        \n",
    "        # Get similarity scores between the target item and all items rated by the user\n",
    "        similarities = self.item_similarity[item_idx, rated_items]\n",
    "        \n",
    "        # Find top k similar items\n",
    "        if len(similarities) > self.k:\n",
    "            top_indices = np.argsort(similarities)[-self.k:]\n",
    "            similarities = similarities[top_indices]\n",
    "            rated_items = rated_items[top_indices]\n",
    "        \n",
    "        # If no similar items, return user's average rating\n",
    "        if len(similarities) == 0 or np.sum(np.abs(similarities)) == 0:\n",
    "            return np.mean(user_ratings[rated_items]) if len(rated_items) > 0 else np.mean(self.train_matrix.data)\n",
    "        \n",
    "        # Calculate weighted average of ratings from similar items\n",
    "        weights = similarities\n",
    "        ratings = user_ratings[rated_items]\n",
    "        \n",
    "        # Ensure we don't divide by zero\n",
    "        sum_weights = np.sum(np.abs(weights))\n",
    "        if sum_weights > 0:\n",
    "            prediction = np.sum(weights * ratings) / sum_weights\n",
    "        else:\n",
    "            prediction = np.mean(ratings)\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return np.clip(prediction, 0.5, 5.0)\n",
    "    \n",
    "    def predict_for_user(self, user_idx, item_indices=None):\n",
    "        \"\"\"\n",
    "        Generate predictions for a user on specified items\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_indices: List of item indices to predict ratings for (default: all items)\n",
    "            \n",
    "        Returns:\n",
    "            Array of predicted ratings\n",
    "        \"\"\"\n",
    "        if item_indices is None:\n",
    "            item_indices = np.arange(self.train_matrix.shape[1])\n",
    "        \n",
    "        predictions = np.array([self.predict(user_idx, item_idx) for item_idx in item_indices])\n",
    "        return predictions\n",
    "\n",
    "class UserBasedCF:\n",
    "    \"\"\"User-based collaborative filtering using cosine similarity\"\"\"\n",
    "    \n",
    "    def __init__(self, k=50):\n",
    "        self.k = k  # Number of similar users to consider\n",
    "        self.train_matrix = None\n",
    "        self.user_similarity = None\n",
    "    \n",
    "    def fit(self, train_matrix, calculate_similarity=True):\n",
    "        \"\"\"\n",
    "        Calculate user-user similarity matrix\n",
    "        \n",
    "        Args:\n",
    "            train_matrix: User-item rating matrix\n",
    "            calculate_similarity: Whether to calculate similarity matrix or use pre-computed one\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train_matrix = train_matrix\n",
    "        \n",
    "        if calculate_similarity:\n",
    "            print(\"Calculating user-user similarity matrix...\")\n",
    "            # Computing user-user similarity is expensive for large datasets\n",
    "            # For better efficiency, we'll normalize the data first\n",
    "            \n",
    "            # Normalize ratings by user means\n",
    "            user_means = np.array([rating.mean() if rating.size > 0 else 0 \n",
    "                                  for rating in [train_matrix[u].data for u in range(train_matrix.shape[0])]])\n",
    "            \n",
    "            # Create a normalized matrix for better similarity calculation\n",
    "            normalized_matrix = train_matrix.copy()\n",
    "            for u in range(train_matrix.shape[0]):\n",
    "                u_ratings = normalized_matrix[u].nonzero()[1]\n",
    "                if len(u_ratings) > 0:\n",
    "                    normalized_matrix[u, u_ratings] = train_matrix[u, u_ratings].toarray()[0] - user_means[u]\n",
    "            \n",
    "            # Calculate user similarities using cosine similarity\n",
    "            # This is memory-intensive for large datasets\n",
    "            self.user_similarity = cosine_similarity(normalized_matrix)\n",
    "            \n",
    "            # Zero out self-similarity to avoid trivial recommendations\n",
    "            np.fill_diagonal(self.user_similarity, 0)\n",
    "            \n",
    "            print(f\"User similarity matrix computed. Shape: {self.user_similarity.shape}\")\n",
    "            print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
    "        else:\n",
    "            print(\"Using pre-computed similarity matrix\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_idx: Item index\n",
    "            \n",
    "        Returns:\n",
    "            Predicted rating\n",
    "        \"\"\"\n",
    "        # Get users who rated the item\n",
    "        item_ratings = self.train_matrix[:, item_idx].toarray().flatten()\n",
    "        users_rated = np.where(item_ratings > 0)[0]\n",
    "        \n",
    "        if len(users_rated) == 0:\n",
    "            # If no user rated this item, return the user's average rating\n",
    "            user_ratings = self.train_matrix[user_idx].data\n",
    "            return np.mean(user_ratings) if len(user_ratings) > 0 else np.mean(self.train_matrix.data)\n",
    "        \n",
    "        # Get similarity scores between current user and users who rated the item\n",
    "        similarities = self.user_similarity[user_idx, users_rated]\n",
    "        \n",
    "        # Find top k similar users\n",
    "        if len(similarities) > self.k:\n",
    "            top_indices = np.argsort(similarities)[-self.k:]\n",
    "            similarities = similarities[top_indices]\n",
    "            users_rated = users_rated[top_indices]\n",
    "        \n",
    "        # If no similar users, return global average\n",
    "        if len(similarities) == 0 or np.sum(np.abs(similarities)) == 0:\n",
    "            return np.mean(self.train_matrix.data)\n",
    "        \n",
    "        # Calculate weighted average of ratings from similar users\n",
    "        weights = similarities\n",
    "        ratings = item_ratings[users_rated]\n",
    "        \n",
    "        # Ensure we don't divide by zero\n",
    "        sum_weights = np.sum(np.abs(weights))\n",
    "        if sum_weights > 0:\n",
    "            prediction = np.sum(weights * ratings) / sum_weights\n",
    "        else:\n",
    "            prediction = np.mean(ratings)\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return np.clip(prediction, 0.5, 5.0)\n",
    "    \n",
    "    def predict_for_user(self, user_idx, item_indices=None):\n",
    "        \"\"\"\n",
    "        Generate predictions for a user on specified items\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_indices: List of item indices to predict ratings for (default: all items)\n",
    "            \n",
    "        Returns:\n",
    "            Array of predicted ratings\n",
    "        \"\"\"\n",
    "        if item_indices is None:\n",
    "            item_indices = np.arange(self.train_matrix.shape[1])\n",
    "        \n",
    "        predictions = np.array([self.predict(user_idx, item_idx) for item_idx in item_indices])\n",
    "        return predictions\n",
    "\n",
    "# --------- 4. MATRIX FACTORIZATION MODELS ---------\n",
    "\n",
    "class SVDModel:\n",
    "    \"\"\"Singular Value Decomposition model for collaborative filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=100, regularization=0.1, n_epochs=20, learning_rate=0.005):\n",
    "        self.n_factors = n_factors\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_avg = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "    \n",
    "    def fit(self, train_matrix, val_matrix=None):\n",
    "        \"\"\"\n",
    "        Train the SVD model using Stochastic Gradient Descent\n",
    "        \n",
    "        Args:\n",
    "            train_matrix: Training user-item rating matrix\n",
    "            val_matrix: Validation user-item rating matrix for early stopping\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        n_users, n_items = train_matrix.shape\n",
    "        \n",
    "        # Initialize model parameters\n",
    "        self.global_avg = np.mean(train_matrix.data)\n",
    "        self.user_biases = np.zeros(n_users)\n",
    "        self.item_biases = np.zeros(n_items)\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "        \n",
    "        # Prepare training data\n",
    "        users, items = train_matrix.nonzero()\n",
    "        ratings = np.array(train_matrix[users, items]).flatten()\n",
    "        \n",
    "        # Prepare validation data if provided\n",
    "        if val_matrix is not None:\n",
    "            val_users, val_items = val_matrix.nonzero()\n",
    "            val_ratings = np.array(val_matrix[val_users, val_items]).flatten()\n",
    "        \n",
    "        best_val_rmse = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        print(\"Training SVD model with SGD...\")\n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Shuffle training data\n",
    "            indices = np.arange(len(users))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # SGD updates\n",
    "            for idx in indices:\n",
    "                u, i, r = users[idx], items[idx], ratings[idx]\n",
    "                \n",
    "                # Compute predicted rating\n",
    "                pred = self.global_avg + self.user_biases[u] + self.item_biases[i] + np.dot(self.user_factors[u], self.item_factors[i])\n",
    "                \n",
    "                # Compute error\n",
    "                error = r - pred\n",
    "                \n",
    "                # Update biases\n",
    "                self.user_biases[u] += self.learning_rate * (error - self.regularization * self.user_biases[u])\n",
    "                self.item_biases[i] += self.learning_rate * (error - self.regularization * self.item_biases[i])\n",
    "                \n",
    "                # Update latent factors\n",
    "                user_factor = self.user_factors[u].copy()\n",
    "                item_factor = self.item_factors[i].copy()\n",
    "                \n",
    "                self.user_factors[u] += self.learning_rate * (error * item_factor - self.regularization * user_factor)\n",
    "                self.item_factors[i] += self.learning_rate * (error * user_factor - self.regularization * item_factor)\n",
    "            \n",
    "            # Compute training RMSE\n",
    "            train_preds = np.array([self.global_avg + self.user_biases[u] + self.item_biases[i] + \n",
    "                                  np.dot(self.user_factors[u], self.item_factors[i]) \n",
    "                                  for u, i in zip(users, items)])\n",
    "            train_rmse = np.sqrt(mean_squared_error(ratings, train_preds))\n",
    "            \n",
    "            # Compute validation RMSE if validation data is provided\n",
    "            if val_matrix is not None:\n",
    "                val_preds = np.array([self.global_avg + self.user_biases[u] + self.item_biases[i] + \n",
    "                                    np.dot(self.user_factors[u], self.item_factors[i]) \n",
    "                                    for u, i in zip(val_users, val_items)])\n",
    "                val_rmse = np.sqrt(mean_squared_error(val_ratings, val_preds))\n",
    "                \n",
    "                # Early stopping check\n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                elif epoch - best_epoch >= 3:  # Stop if no improvement for 3 epochs\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - \"\n",
    "                      f\"Train RMSE: {train_rmse:.4f} - \"\n",
    "                      f\"Val RMSE: {val_rmse:.4f} - \"\n",
    "                      f\"Time: {time.time() - epoch_start:.2f}s\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - \"\n",
    "                      f\"Train RMSE: {train_rmse:.4f} - \"\n",
    "                      f\"Time: {time.time() - epoch_start:.2f}s\")\n",
    "        \n",
    "        print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_indices, item_indices):\n",
    "        \"\"\"\n",
    "        Predict ratings for given user-item pairs\n",
    "        \n",
    "        Args:\n",
    "            user_indices: List of user indices\n",
    "            item_indices: List of item indices\n",
    "            \n",
    "        Returns:\n",
    "            Array of predicted ratings\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(len(user_indices))\n",
    "        \n",
    "        for i, (u, m) in enumerate(zip(user_indices, item_indices)):\n",
    "            # Compute prediction using the learned model\n",
    "            pred = self.global_avg + self.user_biases[u] + self.item_biases[m] + np.dot(self.user_factors[u], self.item_factors[m])\n",
    "            # Clip to valid rating range\n",
    "            predictions[i] = np.clip(pred, 0.5, 5.0)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\"Generate full prediction matrix using the trained model\"\"\"\n",
    "        n_users, n_items = len(self.user_biases), len(self.item_biases)\n",
    "        predictions = np.zeros((n_users, n_items))\n",
    "        \n",
    "        # Compute dot product between all user and item factors\n",
    "        dot_products = np.dot(self.user_factors, self.item_factors.T)\n",
    "        \n",
    "        # Add biases using broadcasting\n",
    "        predictions = (self.global_avg + \n",
    "                      self.user_biases.reshape(-1, 1) + \n",
    "                      self.item_biases.reshape(1, -1) + \n",
    "                      dot_products)\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return np.clip(predictions, 0.5, 5.0)\n",
    "\n",
    "class SVDppModel:\n",
    "    \"\"\"\n",
    "    SVD++ model incorporating implicit feedback\n",
    "    This is a simplified implementation of the full SVD++ algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=100, regularization=0.1, n_epochs=20, learning_rate=0.005):\n",
    "        self.n_factors = n_factors\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_avg = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.implicit_factors = None  # Implicit feedback factors\n",
    "    \n",
    "    def fit(self, train_matrix, val_matrix=None):\n",
    "        \"\"\"\n",
    "        Train the SVD++ model using Stochastic Gradient Descent\n",
    "        \n",
    "        Args:\n",
    "            train_matrix: Training user-item rating matrix\n",
    "            val_matrix: Validation user-item rating matrix for early stopping\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        n_users, n_items = train_matrix.shape\n",
    "        \n",
    "        # Initialize model parameters\n",
    "        self.global_avg = np.mean(train_matrix.data)\n",
    "        self.user_biases = np.zeros(n_users)\n",
    "        self.item_biases = np.zeros(n_items)\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "        self.implicit_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "        \n",
    "        # Prepare training data\n",
    "        users, items = train_matrix.nonzero()\n",
    "        ratings = np.array(train_matrix[users, items]).flatten()\n",
    "        \n",
    "        # Create user to items mapping for implicit feedback\n",
    "        user_to_items = {}\n",
    "        for u, i in zip(users, items):\n",
    "            if u not in user_to_items:\n",
    "                user_to_items[u] = []\n",
    "            user_to_items[u].append(i)\n",
    "        \n",
    "        # Prepare validation data if provided\n",
    "        if val_matrix is not None:\n",
    "            val_users, val_items = val_matrix.nonzero()\n",
    "            val_ratings = np.array(val_matrix[val_users, val_items]).flatten()\n",
    "        \n",
    "        best_val_rmse = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        print(\"Training SVD++ model with SGD...\")\n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Shuffle training data\n",
    "            indices = np.arange(len(users))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # SGD updates\n",
    "            for idx in indices:\n",
    "                u, i, r = users[idx], items[idx], ratings[idx]\n",
    "                \n",
    "                # Get items rated by user u (implicit feedback)\n",
    "                rated_items = user_to_items.get(u, [])\n",
    "                implicit_sum = np.zeros(self.n_factors)\n",
    "                \n",
    "                if rated_items:\n",
    "                    # Compute the sum of implicit factors\n",
    "                    for j in rated_items:\n",
    "                        implicit_sum += self.implicit_factors[j]\n",
    "                    \n",
    "                    # Normalize by the square root of the number of items\n",
    "                    implicit_sum /= np.sqrt(len(rated_items))\n",
    "                \n",
    "                # Compute predicted rating (SVD++ formula)\n",
    "                pred = (self.global_avg + \n",
    "                        self.user_biases[u] + \n",
    "                        self.item_biases[i] + \n",
    "                        np.dot(self.item_factors[i], self.user_factors[u] + implicit_sum))\n",
    "                \n",
    "                # Compute error\n",
    "                error = r - pred\n",
    "                \n",
    "                # Update biases\n",
    "                self.user_biases[u] += self.learning_rate * (error - self.regularization * self.user_biases[u])\n",
    "                self.item_biases[i] += self.learning_rate * (error - self.regularization * self.item_biases[i])\n",
    "                \n",
    "                # Update latent factors\n",
    "                user_factor = self.user_factors[u].copy()\n",
    "                item_factor = self.item_factors[i].copy()\n",
    "                \n",
    "                # Update user factors\n",
    "                self.user_factors[u] += self.learning_rate * (error * item_factor - self.regularization * user_factor)\n",
    "                \n",
    "                # Update item factors\n",
    "                self.item_factors[i] += self.learning_rate * (error * (user_factor + implicit_sum) - self.regularization * item_factor)\n",
    "                \n",
    "                # Update implicit feedback factors\n",
    "                if rated_items:\n",
    "                    factor_update = self.learning_rate * (error * item_factor / np.sqrt(len(rated_items)) - self.regularization * self.implicit_factors)\n",
    "                    for j in rated_items:\n",
    "                        self.implicit_factors[j] += factor_update[j]\n",
    "            \n",
    "            # Compute training RMSE (simplified for efficiency)\n",
    "            sample_size = min(10000, len(users))\n",
    "            sample_indices = np.random.choice(len(users), sample_size, replace=False)\n",
    "            \n",
    "            sample_preds = []\n",
    "            sample_true = []\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                u, i, r = users[idx], items[idx], ratings[idx]\n",
    "                rated_items = user_to_items.get(u, [])\n",
    "                implicit_sum = np.zeros(self.n_factors)\n",
    "                \n",
    "                if rated_items:\n",
    "                    for j in rated_items:\n",
    "                        implicit_sum += self.implicit_factors[j]\n",
    "                    implicit_sum /= np.sqrt(len(rated_items))\n",
    "                \n",
    "                pred = (self.global_avg + \n",
    "                        self.user_biases[u] + \n",
    "                        self.item_biases[i] + \n",
    "                        np.dot(self.item_factors[i], self.user_factors[u] + implicit_sum))\n",
    "                \n",
    "                sample_preds.append(np.clip(pred, 0.5, 5.0))\n",
    "                sample_true.append(r)\n",
    "            \n",
    "            train_rmse = np.sqrt(mean_squared_error(sample_true, sample_preds))\n",
    "            \n",
    "            # Compute validation RMSE if validation data is provided\n",
    "            if val_matrix is not None:\n",
    "                val_sample_size = min(10000, len(val_users))\n",
    "                val_sample_indices = np.random.choice(len(val_users), val_sample_size, replace=False)\n",
    "                \n",
    "                val_sample_preds = []\n",
    "                val_sample_true = []\n",
    "                \n",
    "                for idx in val_sample_indices:\n",
    "                    u, i, r = val_users[idx], val_items[idx], val_ratings[idx]\n",
    "                    rated_items = user_to_items.get(u, [])\n",
    "                    implicit_sum = np.zeros(self.n_factors)\n",
    "                    \n",
    "                    if rated_items:\n",
    "                        for j in rated_items:\n",
    "                            implicit_sum += self.implicit_factors[j]\n",
    "                        implicit_sum /= np.sqrt(len(rated_items))\n",
    "                    \n",
    "                    pred = (self.global_avg + \n",
    "                            self.user_biases[u] + \n",
    "                            self.item_biases[i] + \n",
    "                            np.dot(self.item_factors[i], self.user_factors[u] + implicit_sum))\n",
    "                    \n",
    "                    val_sample_preds.append(np.clip(pred, 0.5, 5.0))\n",
    "                    val_sample_true.append(r)\n",
    "                \n",
    "                val_rmse = np.sqrt(mean_squared_error(val_sample_true, val_sample_preds))\n",
    "                \n",
    "                # Early stopping check\n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                elif epoch - best_epoch >= 3:  # Stop if no improvement for 3 epochs\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - \"\n",
    "                      f\"Train RMSE: {train_rmse:.4f} - \"\n",
    "                      f\"Val RMSE: {val_rmse:.4f} - \"\n",
    "                      f\"Time: {time.time() - epoch_start:.2f}s\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - \"\n",
    "                      f\"Train RMSE: {train_rmse:.4f} - \"\n",
    "                      f\"Time: {time.time() - epoch_start:.2f}s\")\n",
    "        \n",
    "        print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx, user_to_items=None):\n",
    "        \"\"\"\n",
    "        Predict rating for a single user-item pair\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_idx: Item index\n",
    "            user_to_items: Dictionary mapping user indices to list of rated item indices\n",
    "            \n",
    "        Returns:\n",
    "            Predicted rating\n",
    "        \"\"\"\n",
    "        # Initialize implicit sum\n",
    "        implicit_sum = np.zeros(self.n_factors)\n",
    "        \n",
    "        # Compute implicit feedback component if rated items are provided\n",
    "        if user_to_items is not None and user_idx in user_to_items:\n",
    "            rated_items = user_to_items[user_idx]\n",
    "            if rated_items:\n",
    "                for j in rated_items:\n",
    "                    implicit_sum += self.implicit_factors[j]\n",
    "                implicit_sum /= np.sqrt(len(rated_items))\n",
    "        \n",
    "        # Compute prediction using SVD++ formula\n",
    "        pred = (self.global_avg + \n",
    "                self.user_biases[user_idx] + \n",
    "                self.item_biases[item_idx] + \n",
    "                np.dot(self.item_factors[item_idx], self.user_factors[user_idx] + implicit_sum))\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return np.clip(pred, 0.5, 5.0)\n",
    "    \n",
    "    def predict_for_user(self, user_idx, item_indices=None, train_matrix=None):\n",
    "        \"\"\"\n",
    "        Generate predictions for a user on specified items\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_indices: List of item indices to predict ratings for (default: all items)\n",
    "            train_matrix: Training matrix for extracting implicit feedback information\n",
    "            \n",
    "        Returns:\n",
    "            Array of predicted ratings\n",
    "        \"\"\"\n",
    "        if item_indices is None:\n",
    "            item_indices = np.arange(len(self.item_biases))\n",
    "        \n",
    "        # Extract rated items for the user from train_matrix\n",
    "        user_to_items = {}\n",
    "        if train_matrix is not None:\n",
    "            rated_items = train_matrix[user_idx].nonzero()[1]\n",
    "            user_to_items[user_idx] = rated_items\n",
    "        \n",
    "        predictions = np.zeros(len(item_indices))\n",
    "        \n",
    "        # Compute implicit sum once for efficiency\n",
    "        implicit_sum = np.zeros(self.n_factors)\n",
    "        if user_to_items and user_idx in user_to_items:\n",
    "            rated_items = user_to_items[user_idx]\n",
    "            if len(rated_items) > 0:\n",
    "                for j in rated_items:\n",
    "                    implicit_sum += self.implicit_factors[j]\n",
    "                implicit_sum /= np.sqrt(len(rated_items))\n",
    "        \n",
    "        # Generate predictions for each item\n",
    "        for i, item_idx in enumerate(item_indices):\n",
    "            # Compute prediction using SVD++ formula\n",
    "            pred = (self.global_avg + \n",
    "                    self.user_biases[user_idx] + \n",
    "                    self.item_biases[item_idx] + \n",
    "                    np.dot(self.item_factors[item_idx], self.user_factors[user_idx] + implicit_sum))\n",
    "            \n",
    "            # Clip to valid rating range\n",
    "            predictions[i] = np.clip(pred, 0.5, 5.0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# --------- 5. HYBRID RECOMMENDATION MODEL ---------\n",
    "\n",
    "class HybridRecommender:\n",
    "    \"\"\"\n",
    "    Hybrid recommendation model that combines multiple recommendation models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid recommender\n",
    "        \n",
    "        Args:\n",
    "            models: List of recommendation models\n",
    "            weights: List of weights for each model (default: equal weights)\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.weights = weights if weights is not None else [1.0/len(models)] * len(models)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        self.weights = np.array(self.weights) / sum(self.weights)\n",
    "    \n",
    "    def predict(self, user_idx, item_idx, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate a prediction by combining predictions from multiple models\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_idx: Item index\n",
    "            **kwargs: Additional arguments to pass to the models\n",
    "            \n",
    "        Returns:\n",
    "            Weighted prediction\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            if hasattr(model, 'predict'):\n",
    "                if 'user_idx' in kwargs and 'item_idx' in kwargs:\n",
    "                    pred = model.predict(kwargs['user_idx'], kwargs['item_idx'])\n",
    "                else:\n",
    "                    pred = model.predict(user_idx, item_idx)\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Weighted average of predictions\n",
    "        if predictions:\n",
    "            return np.average(predictions, weights=self.weights[:len(predictions)])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def predict_for_user(self, user_idx, item_indices=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate predictions for a user on multiple items\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_indices: List of item indices to predict ratings for\n",
    "            **kwargs: Additional arguments to pass to the models\n",
    "            \n",
    "        Returns:\n",
    "            Array of weighted predictions\n",
    "        \"\"\"\n",
    "        if item_indices is None:\n",
    "            # Assume all models have the same number of items\n",
    "            for model in self.models:\n",
    "                if hasattr(model, 'item_biases'):\n",
    "                    item_indices = np.arange(len(model.item_biases))\n",
    "                    break\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            if hasattr(model, 'predict_for_user'):\n",
    "                preds = model.predict_for_user(user_idx, item_indices, **kwargs)\n",
    "                all_predictions.append(preds)\n",
    "        \n",
    "        # Weighted average of predictions\n",
    "        if all_predictions:\n",
    "            weighted_preds = np.zeros(len(item_indices))\n",
    "            for i, preds in enumerate(all_predictions):\n",
    "                weighted_preds += self.weights[i] * preds\n",
    "            return weighted_preds\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def recommend_items(self, user_idx, n=10, exclude_rated=True, train_matrix=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Recommend top N items for a user\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            n: Number of recommendations\n",
    "            exclude_rated: Whether to exclude already rated items\n",
    "            train_matrix: Training matrix for excluding rated items\n",
    "            **kwargs: Additional arguments to pass to the models\n",
    "            \n",
    "        Returns:\n",
    "            List of top N recommended item indices\n",
    "        \"\"\"\n",
    "        # Get predictions for all items\n",
    "        predictions = self.predict_for_user(user_idx, **kwargs)\n",
    "        \n",
    "        if exclude_rated and train_matrix is not None:\n",
    "            # Get items the user has already rated\n",
    "            rated_items = train_matrix[user_idx].nonzero()[1]\n",
    "            # Set predictions for rated items to -inf to exclude them\n",
    "            predictions[rated_items] = -np.inf\n",
    "        \n",
    "        # Get top N items\n",
    "        top_items = np.argsort(predictions)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "\n",
    "# --------- 6. RECOMMENDATION SYSTEM MANAGER ---------\n",
    "\n",
    "class MovieRecommendationSystem:\n",
    "    \"\"\"\n",
    "    Main recommendation system manager that handles model training, evaluation, and recommendation generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"processed_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation system\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing processed data files\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        self.data = load_processed_data(data_dir)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.evaluation_results = []\n",
    "    \n",
    "    def train_models(self, models_to_train=None):\n",
    "        \"\"\"\n",
    "        Train multiple recommendation models\n",
    "        \n",
    "        Args:\n",
    "            models_to_train: List of model names to train (default: all models)\n",
    "        \"\"\"\n",
    "        available_models = {\n",
    "            'global_avg': self._train_global_avg,\n",
    "            'user_item_bias': self._train_user_item_bias,\n",
    "            'item_cf': self._train_item_cf,\n",
    "            'user_cf': self._train_user_cf,\n",
    "            'svd': self._train_svd,\n",
    "            'svdpp': self._train_svdpp,\n",
    "            'hybrid': self._train_hybrid\n",
    "        }\n",
    "        \n",
    "        if models_to_train is None:\n",
    "            models_to_train = list(available_models.keys())\n",
    "        \n",
    "        for model_name in models_to_train:\n",
    "            if model_name in available_models:\n",
    "                print(f\"\\n=== Training {model_name} model ===\")\n",
    "                self.models[model_name] = available_models[model_name]()\n",
    "            else:\n",
    "                print(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    def _train_global_avg(self):\n",
    "        \"\"\"Train global average baseline model\"\"\"\n",
    "        model = GlobalAverageModel()\n",
    "        return model.fit(self.data['train_matrix'])\n",
    "    \n",
    "    def _train_user_item_bias(self):\n",
    "        \"\"\"Train user-item bias baseline model\"\"\"\n",
    "        model = UserItemBiasModel()\n",
    "        return model.fit(self.data['train_matrix'])\n",
    "    \n",
    "    def _train_item_cf(self):\n",
    "        \"\"\"Train item-based collaborative filtering model\"\"\"\n",
    "        model = ItemBasedCF(k=50)\n",
    "        return model.fit(self.data['train_matrix'])\n",
    "    \n",
    "    def _train_user_cf(self):\n",
    "        \"\"\"Train user-based collaborative filtering model\"\"\"\n",
    "        model = UserBasedCF(k=50)\n",
    "        return model.fit(self.data['train_matrix'])\n",
    "    \n",
    "    def _train_svd(self):\n",
    "        \"\"\"Train SVD model\"\"\"\n",
    "        model = SVDModel(n_factors=50, regularization=0.1, n_epochs=20, learning_rate=0.005)\n",
    "        return model.fit(self.data['train_matrix'], self.data['val_matrix'])\n",
    "    \n",
    "    def _train_svdpp(self):\n",
    "        \"\"\"Train SVD++ model\"\"\"\n",
    "        model = SVDppModel(n_factors=50, regularization=0.1, n_epochs=20, learning_rate=0.005)\n",
    "        return model.fit(self.data['train_matrix'], self.data['val_matrix'])\n",
    "    \n",
    "    def _train_hybrid(self):\n",
    "        \"\"\"Train hybrid model combining multiple models\"\"\"\n",
    "        models = []\n",
    "        weights = []\n",
    "        \n",
    "        # Add trained models to the hybrid\n",
    "        if 'user_item_bias' in self.models:\n",
    "            models.append(self.models['user_item_bias'])\n",
    "            weights.append(0.1)\n",
    "        \n",
    "        if 'item_cf' in self.models:\n",
    "            models.append(self.models['item_cf'])\n",
    "            weights.append(0.3)\n",
    "        \n",
    "        if 'svd' in self.models:\n",
    "            models.append(self.models['svd'])\n",
    "            weights.append(0.6)\n",
    "        \n",
    "        if not models:\n",
    "            print(\"No component models available for hybrid. Training defaults...\")\n",
    "            models = [self._train_user_item_bias(), self._train_item_cf(), self._train_svd()]\n",
    "            weights = [0.1, 0.3, 0.6]\n",
    "        \n",
    "        print(f\"Creating hybrid model with {len(models)} component models\")\n",
    "        return HybridRecommender(models, weights)\n",
    "    \n",
    "    def evaluate_models(self, test_sample_size=10000):\n",
    "        \"\"\"\n",
    "        Evaluate all trained models on test data\n",
    "        \n",
    "        Args:\n",
    "            test_sample_size: Number of test ratings to sample (for efficiency)\n",
    "        \n",
    "        Returns:\n",
    "            List of evaluation results for each model\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Evaluating Models ===\")\n",
    "        \n",
    "        # Sample test data\n",
    "        test = self.data['test']\n",
    "        if len(test) > test_sample_size:\n",
    "            test_sample = test.sample(n=test_sample_size, random_state=42)\n",
    "        else:\n",
    "            test_sample = test\n",
    "        \n",
    "        user_indices = [self.data['user_map'][uid] for uid in test_sample['userId']]\n",
    "        item_indices = [self.data['movie_map'][mid] for mid in test_sample['movieId']]\n",
    "        true_ratings = test_sample['rating'].values\n",
    "        \n",
    "        self.evaluation_results = []\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {model_name} model...\")\n",
    "            \n",
    "            # Generate predictions\n",
    "            if model_name == 'user_cf' or model_name == 'item_cf':\n",
    "                # These models don't support batch prediction\n",
    "                predictions = []\n",
    "                for u, i in zip(user_indices, item_indices):\n",
    "                    try:\n",
    "                        pred = model.predict(u, i)\n",
    "                        predictions.append(pred)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                        predictions.append(3.0)  # Default prediction\n",
    "                predictions = np.array(predictions)\n",
    "            else:\n",
    "                predictions = model.predict(user_indices, item_indices)\n",
    "            \n",
    "            # Evaluate\n",
    "            result = evaluate_model(predictions, true_ratings, name=model_name)\n",
    "            self.evaluation_results.append(result)\n",
    "        \n",
    "        # Find the best model\n",
    "        if self.evaluation_results:\n",
    "            # Find model with lowest RMSE\n",
    "            best_idx = np.argmin([r['rmse'] for r in self.evaluation_results])\n",
    "            best_model_name = self.evaluation_results[best_idx]['model']\n",
    "            self.best_model = self.models[best_model_name]\n",
    "            \n",
    "            print(f\"\\nBest performing model: {best_model_name} with RMSE: {self.evaluation_results[best_idx]['rmse']:.4f}\")\n",
    "        \n",
    "        return self.evaluation_results\n",
    "    \n",
    "    def recommend_for_user(self, user_id, n=10, model_name=None, exclude_rated=True):\n",
    "        \"\"\"\n",
    "        Generate movie recommendations for a user\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID (raw ID, not matrix index)\n",
    "            n: Number of recommendations\n",
    "            model_name: Name of model to use (default: best model)\n",
    "            exclude_rated: Whether to exclude already rated movies\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with recommended movies and their details\n",
    "        \"\"\"\n",
    "        if model_name is None and self.best_model is not None:\n",
    "            model = self.best_model\n",
    "            model_name = next(name for name, m in self.models.items() if m is self.best_model)\n",
    "        elif model_name in self.models:\n",
    "            model = self.models[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' not found or no best model available\")\n",
    "        \n",
    "        # Convert user ID to matrix index\n",
    "        if user_id in self.data['user_map']:\n",
    "            user_idx = self.data['user_map'][user_id]\n",
    "        else:\n",
    "            raise ValueError(f\"User ID {user_id} not found in the dataset\")\n",
    "        \n",
    "        print(f\"Generating recommendations for user {user_id} using {model_name} model...\")\n",
    "        \n",
    "        # Get recommendations\n",
    "        if hasattr(model, 'recommend_items'):\n",
    "            # For hybrid model\n",
    "            top_items = model.recommend_items(\n",
    "                user_idx, n=n, exclude_rated=exclude_rated, \n",
    "                train_matrix=self.data['train_matrix']\n",
    "            )\n",
    "        elif model_name in ['user_cf', 'item_cf']:\n",
    "            # For CF models\n",
    "            predictions = model.predict_for_user(user_idx)\n",
    "            if exclude_rated:\n",
    "                # Exclude already rated items\n",
    "                rated_items = self.data['train_matrix'][user_idx].nonzero()[1]\n",
    "                predictions[rated_items] = -np.inf\n",
    "            top_items = np.argsort(predictions)[::-1][:n]\n",
    "        else:\n",
    "            # For other models\n",
    "            if hasattr(model, 'predict_all'):\n",
    "                # For models that can generate all predictions at once\n",
    "                predictions = model.predict_all()[user_idx]\n",
    "            else:\n",
    "                # For models that predict one at a time\n",
    "                all_items = np.arange(self.data['train_matrix'].shape[1])\n",
    "                predictions = np.array([model.predict(user_idx, i) for i in all_items])\n",
    "                \n",
    "            if exclude_rated:\n",
    "                # Exclude already rated items\n",
    "                rated_items = self.data['train_matrix'][user_idx].nonzero()[1]\n",
    "                predictions[rated_items] = -np.inf\n",
    "            \n",
    "            top_items = np.argsort(predictions)[::-1][:n]\n",
    "        \n",
    "        # Convert matrix indices back to movie IDs\n",
    "        movie_ids = [self.data['movie_map_rev'][idx] for idx in top_items]\n",
    "        \n",
    "        # Get movie details\n",
    "        movies_df = self.data['movies']\n",
    "        recommended_movies = movies_df[movies_df['movieId'].isin(movie_ids)].copy()\n",
    "        \n",
    "        # Add predicted ratings\n",
    "        if hasattr(model, 'predict'):\n",
    "            predicted_ratings = [model.predict(user_idx, item_idx) for item_idx in top_items]\n",
    "            movie_id_to_rating = dict(zip(movie_ids, predicted_ratings))\n",
    "            recommended_movies['predicted_rating'] = recommended_movies['movieId'].map(movie_id_to_rating)\n",
    "        \n",
    "        # Sort by predicted rating\n",
    "        if 'predicted_rating' in recommended_movies.columns:\n",
    "            recommended_movies = recommended_movies.sort_values('predicted_rating', ascending=False)\n",
    "        \n",
    "        return recommended_movies[['movieId', 'title', 'genres', 'predicted_rating']].reset_index(drop=True)\n",
    "    \n",
    "    def get_similar_movies(self, movie_id, n=10, model_name='item_cf'):\n",
    "        \"\"\"\n",
    "        Find similar movies to a given movie\n",
    "        \n",
    "        Args:\n",
    "            movie_id: Movie ID (raw ID, not matrix index)\n",
    "            n: Number of similar movies to return\n",
    "            model_name: Name of model to use (only 'item_cf' and 'svd' supported)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with similar movies and their details\n",
    "        \"\"\"\n",
    "        if model_name not in ['item_cf', 'svd']:\n",
    "            raise ValueError(\"Only 'item_cf' and 'svd' models support similar movie recommendations\")\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model '{model_name}' not found\")\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        # Convert movie ID to matrix index\n",
    "        if movie_id in self.data['movie_map']:\n",
    "            movie_idx = self.data['movie_map'][movie_id]\n",
    "        else:\n",
    "            raise ValueError(f\"Movie ID {movie_id} not found in the dataset\")\n",
    "        \n",
    "        print(f\"Finding movies similar to movie {movie_id} using {model_name} model...\")\n",
    "        \n",
    "        # Get similar movies\n",
    "        if model_name == 'item_cf':\n",
    "            # Get similarity scores\n",
    "            similarity_scores = model.item_similarity[movie_idx]\n",
    "            # Get top similar movies\n",
    "            similar_indices = np.argsort(similarity_scores)[::-1][1:n+1]  # Skip the first one (self)\n",
    "            similarities = similarity_scores[similar_indices]\n",
    "        elif model_name == 'svd':\n",
    "            # Use item factors to compute similarity\n",
    "            target_factors = model.item_factors[movie_idx]\n",
    "            # Compute cosine similarity with all other items\n",
    "            similarities = cosine_similarity([target_factors], model.item_factors)[0]\n",
    "            # Get top similar movies\n",
    "            similar_indices = np.argsort(similarities)[::-1][1:n+1]  # Skip the first one (self)\n",
    "            similarities = similarities[similar_indices]\n",
    "        \n",
    "        # Convert matrix indices back to movie IDs\n",
    "        movie_ids = [self.data['movie_map_rev'][idx] for idx in similar_indices]\n",
    "        \n",
    "        # Get movie details\n",
    "        movies_df = self.data['movies']\n",
    "        similar_movies = movies_df[movies_df['movieId'].isin(movie_ids)].copy()\n",
    "        \n",
    "        # Add similarity scores\n",
    "        movie_id_to_similarity = dict(zip(movie_ids, similarities))\n",
    "        similar_movies['similarity_score'] = similar_movies['movieId'].map(movie_id_to_similarity)\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similar_movies = similar_movies.sort_values('similarity_score', ascending=False)\n",
    "        \n",
    "        return similar_movies[['movieId', 'title', 'genres', 'similarity_score']].reset_index(drop=True)\n",
    "    \n",
    "    def plot_evaluation_results(self):\n",
    "        \"\"\"Plot evaluation results for all models\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"No evaluation results to plot. Run evaluate_models() first.\")\n",
    "            return\n",
    "        \n",
    "        # Create dataframe from evaluation results\n",
    "        results_df = pd.DataFrame(self.evaluation_results)\n",
    "        \n",
    "        # Plot RMSE\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        barplot = sns.barplot(x='model', y='rmse', data=results_df)\n",
    "        plt.title('RMSE by Model', fontsize=16)\n",
    "        plt.xlabel('Model', fontsize=14)\n",
    "        plt.ylabel('RMSE (lower is better)', fontsize=14)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for patch in barplot.patches:\n",
    "            barplot.annotate(f\"{patch.get_height():.4f}\",\n",
    "                          (patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "                          ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot accuracy within thresholds\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        results_melted = pd.melt(results_df, id_vars=['model'], value_vars=['within_05', 'within_1'],\n",
    "                              var_name='threshold', value_name='accuracy')\n",
    "        results_melted['threshold'] = results_melted['threshold'].map({\n",
    "            'within_05': 'Within 0.5 stars',\n",
    "            'within_1': 'Within 1.0 stars'\n",
    "        })\n",
    "        \n",
    "        barplot = sns.barplot(x='model', y='accuracy', hue='threshold', data=results_melted)\n",
    "        plt.title('Prediction Accuracy by Model', fontsize=16)\n",
    "        plt.xlabel('Model', fontsize=14)\n",
    "        plt.ylabel('Accuracy (higher is better)', fontsize=14)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Threshold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for patch in barplot.patches:\n",
    "            barplot.annotate(f\"{patch.get_height():.2%}\",\n",
    "                          (patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "                          ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main function to run the recommendation system\n",
    "def main():\n",
    "    # Create recommendation system\n",
    "    rec_system = MovieRecommendationSystem()\n",
    "    \n",
    "    # Train models\n",
    "    models_to_train = ['global_avg', 'user_item_bias', 'item_cf', 'svd', 'hybrid']\n",
    "    rec_system.train_models(models_to_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    rec_system.evaluate_models()\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    rec_system.plot_evaluation_results()\n",
    "    \n",
    "    # Generate recommendations for a sample user\n",
    "    user_id = 1  # Replace with a valid user ID\n",
    "    recommendations = rec_system.recommend_for_user(user_id, n=10)\n",
    "    print(\"\\nTop 10 movie recommendations for user:\", user_id)\n",
    "    print(recommendations)\n",
    "    \n",
    "    # Find similar movies to a sample movie\n",
    "    movie_id = 1  # Replace with a valid movie ID\n",
    "    similar_movies = rec_system.get_similar_movies(movie_id, n=10)\n",
    "    print(\"\\nTop 10 movies similar to movie:\", movie_id)\n",
    "    print(similar_movies)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selected_topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
