{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8747afb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 23:48:03 WARN Utils: Your hostname, lenovo-server resolves to a loopback address: 127.0.1.1; using 192.168.100.30 instead (on interface eno1)\n",
      "25/04/14 23:48:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 23:48:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-04-14 23:48:04,746 - MovieLensRecommender - INFO - Spark session initialized with 8g memory\n",
      "2025-04-14 23:48:04,747 - MovieLensRecommender - INFO - Loading data with PySpark...\n",
      "2025-04-14 23:48:49,776 - MovieLensRecommender - INFO - Data loaded in 45.03 seconds\n",
      "2025-04-14 23:48:49,777 - MovieLensRecommender - INFO - Filtered dataset: 200,948 users, 43,884 movies, 31,921,467 ratings\n",
      "2025-04-14 23:48:49,777 - MovieLensRecommender - INFO - Processing movie features...\n",
      "2025-04-14 23:48:50,979 - MovieLensRecommender - INFO - Created 20 genre features and genre vectors\n",
      "2025-04-14 23:48:50,980 - MovieLensRecommender - INFO - Splitting data into train(70.0%)/val(15.0%)/test(15.0%)...\n",
      "2025-04-14 23:50:15,445 - MovieLensRecommender - INFO - Training set: 22,343,094 ratings (69.99%)\n",
      "2025-04-14 23:50:15,446 - MovieLensRecommender - INFO - Validation set: 4,787,886 ratings (15.00%)\n",
      "2025-04-14 23:50:15,447 - MovieLensRecommender - INFO - Test set: 4,790,487 ratings (15.01%)\n",
      "2025-04-14 23:50:15,448 - MovieLensRecommender - INFO - Training baseline models...\n",
      "2025-04-14 23:50:15,942 - MovieLensRecommender - INFO - Global mean rating: 3.5421\n",
      "2025-04-14 23:50:15,946 - MovieLensRecommender - INFO - Global Mean Model Performance:\n",
      "2025-04-14 23:50:19,418 - MovieLensRecommender - INFO - RMSE: 1.0590, MAE: 0.8381, R2: -0.0000\n",
      "2025-04-14 23:50:19,484 - MovieLensRecommender - INFO - Bias Model Performance:\n",
      "2025-04-14 23:50:38,622 - MovieLensRecommender - INFO - RMSE: 0.8788, MAE: 0.6680, R2: 0.3113\n",
      "2025-04-14 23:50:38,623 - MovieLensRecommender - INFO - Baseline models trained and evaluated in 23.17 seconds\n",
      "2025-04-14 23:50:42,935 - MovieLensRecommender - INFO - Training ALS model (rank=50, regParam=0.1, maxIter=10, implicit=False)...\n",
      "2025-04-14 23:53:20,152 - MovieLensRecommender - INFO - ALS Model Performance:  \n",
      "2025-04-14 23:54:08,292 - MovieLensRecommender - INFO - RMSE: 0.8075, MAE: 0.6276, R2: 0.4186\n",
      "2025-04-14 23:54:08,295 - MovieLensRecommender - INFO - ALS model trained in 205.36 seconds\n",
      "2025-04-14 23:54:08,297 - MovieLensRecommender - INFO - Extracting ALS model parameters...\n",
      "2025-04-14 23:54:11,036 - MovieLensRecommender - INFO - Extracted factors for 200948 users and 43875 items\n",
      "2025-04-14 23:54:11,062 - MovieLensRecommender - INFO - Creating hybrid model with ALS weight=0.7\n",
      "2025-04-14 23:54:11,130 - MovieLensRecommender - INFO - Hybrid Model Performance:\n",
      "2025-04-14 23:55:10,867 - MovieLensRecommender - INFO - RMSE: 0.8151, MAE: 0.6282, R2: 0.4076\n",
      "2025-04-14 23:55:10,868 - MovieLensRecommender - INFO - Training content-based filtering model...\n",
      "2025-04-14 23:55:31,359 - MovieLensRecommender - ERROR - Error in recommendation pipeline: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_81042/4095781356.py\", line 1127, in main\n",
      "    content_params = train_content_based_model(\n",
      "  File \"/tmp/ipykernel_81042/4095781356.py\", line 743, in train_content_based_model\n",
      "    total = sum([r * c for r, c in zip(genre_ratings, genre_counts) if c > 0])\n",
      "  File \"/home/ir-proj/anaconda3/envs/sltd/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ir-proj/anaconda3/envs/sltd/lib/python3.9/site-packages/pyspark/sql/functions.py\", line 866, in sum\n",
      "    return _invoke_function_over_columns(\"sum\", col)\n",
      "  File \"/home/ir-proj/anaconda3/envs/sltd/lib/python3.9/site-packages/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/ir-proj/anaconda3/envs/sltd/lib/python3.9/site-packages/pyspark/sql/functions.py\", line 105, in <genexpr>\n",
      "    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n",
      "  File \"/home/ir-proj/anaconda3/envs/sltd/lib/python3.9/site-packages/pyspark/sql/column.py\", line 65, in _to_java_column\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n",
      "2025-04-14 23:55:32,856 - MovieLensRecommender - INFO - Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Spark-only implementation of recommendation models for MovieLens 32M\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit, expr, udf, collect_list, sum, avg, desc, array, explode, first\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType, ArrayType, StructType, StructField, StringType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "import pickle\n",
    "import joblib\n",
    "import gc\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"recommender.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"MovieLensRecommender\")\n",
    "\n",
    "# Initialize Spark session\n",
    "def init_spark(app_name=\"MovieLens_Recommender\", memory=\"5g\"):\n",
    "    \"\"\"Initialize a Spark session with specified memory allocation\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", memory) \\\n",
    "        .config(\"spark.executor.memory\", memory) \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbosity\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    logger.info(f\"Spark session initialized with {memory} memory\")\n",
    "    return spark\n",
    "\n",
    "# Load data using PySpark\n",
    "def load_data(spark, data_dir=\"ml-32m\", min_ratings=5):\n",
    "    \"\"\"Load the MovieLens dataset into Spark DataFrames\"\"\"\n",
    "    logger.info(\"Loading data with PySpark...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings_df = spark.read.csv(\n",
    "        os.path.join(data_dir, 'ratings.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    # Load movies\n",
    "    movies_df = spark.read.csv(\n",
    "        os.path.join(data_dir, 'movies.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    # Filter users and movies with minimum ratings\n",
    "    user_counts = ratings_df.groupBy(\"userId\").count().filter(col(\"count\") >= min_ratings)\n",
    "    active_users = user_counts.select(\"userId\")\n",
    "    \n",
    "    movie_counts = ratings_df.groupBy(\"movieId\").count().filter(col(\"count\") >= min_ratings)\n",
    "    active_movies = movie_counts.select(\"movieId\")\n",
    "    \n",
    "    # Join to get filtered ratings\n",
    "    filtered_ratings = ratings_df.join(active_users, \"userId\") \\\n",
    "                               .join(active_movies, \"movieId\")\n",
    "    \n",
    "    # Get stats about the filtered dataset\n",
    "    n_users = active_users.count()\n",
    "    n_movies = active_movies.count()\n",
    "    n_ratings = filtered_ratings.count()\n",
    "    \n",
    "    logger.info(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    logger.info(f\"Filtered dataset: {n_users:,} users, {n_movies:,} movies, {n_ratings:,} ratings\")\n",
    "    \n",
    "    return filtered_ratings, movies_df, n_users, n_movies, active_users, active_movies\n",
    "\n",
    "# Process movie features\n",
    "def process_movie_features(spark, movies_df):\n",
    "    \"\"\"Extract and process movie features including genres\"\"\"\n",
    "    logger.info(\"Processing movie features...\")\n",
    "    \n",
    "    # Extract year from title\n",
    "    movies_df = movies_df.withColumn(\n",
    "        \"year\", \n",
    "        expr(\"CAST(regexp_extract(title, '\\\\((\\\\d{4})\\\\)', 1) AS INT)\")\n",
    "    )\n",
    "    \n",
    "    # Clean title (remove year)\n",
    "    movies_df = movies_df.withColumn(\n",
    "        \"clean_title\",\n",
    "        expr(\"regexp_replace(title, '\\\\s*\\\\(\\\\d{4}\\\\)$', '')\")\n",
    "    )\n",
    "    \n",
    "    # Get all unique genres\n",
    "    genres_list = movies_df.select(\"genres\").distinct().rdd.flatMap(lambda x: x[0].split('|')).collect()\n",
    "    all_genres = sorted(list(set(genres_list)))\n",
    "    \n",
    "    if '(no genres listed)' in all_genres:\n",
    "        all_genres.remove('(no genres listed)')\n",
    "    \n",
    "    # Create genre feature columns\n",
    "    for genre in all_genres:\n",
    "        movies_df = movies_df.withColumn(\n",
    "            f\"genre_{genre}\", \n",
    "            (col(\"genres\").contains(genre)).cast(IntegerType())\n",
    "        )\n",
    "    \n",
    "    # Create vector of genre features for content-based filtering\n",
    "    genre_columns = [f\"genre_{genre}\" for genre in all_genres]\n",
    "    assembler = VectorAssembler(inputCols=genre_columns, outputCol=\"genre_vector\")\n",
    "    movies_df = assembler.transform(movies_df)\n",
    "    \n",
    "    logger.info(f\"Created {len(all_genres)} genre features and genre vectors\")\n",
    "    return movies_df, all_genres\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "def train_val_test_split(ratings_df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    \"\"\"Split ratings into training, validation, and test sets\"\"\"\n",
    "    # Ensure ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "    \n",
    "    logger.info(f\"Splitting data into train({train_ratio:.1%})/val({val_ratio:.1%})/test({test_ratio:.1%})...\")\n",
    "    \n",
    "    # Split the data\n",
    "    train_data, temp_data = ratings_df.randomSplit([train_ratio, val_ratio + test_ratio], seed=seed)\n",
    "    \n",
    "    # Adjust validation ratio\n",
    "    val_adjusted_ratio = val_ratio / (val_ratio + test_ratio)\n",
    "    val_data, test_data = temp_data.randomSplit([val_adjusted_ratio, 1.0 - val_adjusted_ratio], seed=seed)\n",
    "    \n",
    "    # Cache the datasets\n",
    "    train_data.cache()\n",
    "    val_data.cache()\n",
    "    test_data.cache()\n",
    "    \n",
    "    # Display split sizes\n",
    "    train_count = train_data.count()\n",
    "    val_count = val_data.count()\n",
    "    test_count = test_data.count()\n",
    "    total = train_count + val_count + test_count\n",
    "    \n",
    "    logger.info(f\"Training set: {train_count:,} ratings ({train_count/total*100:.2f}%)\")\n",
    "    logger.info(f\"Validation set: {val_count:,} ratings ({val_count/total*100:.2f}%)\")\n",
    "    logger.info(f\"Test set: {test_count:,} ratings ({test_count/total*100:.2f}%)\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Function to evaluate with PySpark\n",
    "def evaluate_spark_model(predictions, truth_col=\"rating\", pred_col=\"prediction\"):\n",
    "    \"\"\"Calculate RMSE, MAE and R2 for model predictions using Spark\"\"\"\n",
    "    evaluator_rmse = RegressionEvaluator(\n",
    "        metricName=\"rmse\", \n",
    "        labelCol=truth_col, \n",
    "        predictionCol=pred_col\n",
    "    )\n",
    "    \n",
    "    evaluator_mae = RegressionEvaluator(\n",
    "        metricName=\"mae\", \n",
    "        labelCol=truth_col, \n",
    "        predictionCol=pred_col\n",
    "    )\n",
    "    \n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    \n",
    "    # Calculate R-squared using summary statistics - fixed version\n",
    "    stats = predictions.agg(\n",
    "        lit(predictions.count()).alias(\"count\"),\n",
    "        sum(col(truth_col)).alias(\"sum_y\"),\n",
    "        sum(col(truth_col) * col(truth_col)).alias(\"sum_y_squared\"),\n",
    "        sum((col(truth_col) - col(pred_col)) * (col(truth_col) - col(pred_col))).alias(\"sum_squared_error\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    count = stats[\"count\"]\n",
    "    sum_y = stats[\"sum_y\"]\n",
    "    sum_y_squared = stats[\"sum_y_squared\"]\n",
    "    sum_squared_error = stats[\"sum_squared_error\"]\n",
    "    \n",
    "    y_mean = sum_y / count\n",
    "    total_sum_squares = sum_y_squared - (sum_y * sum_y) / count\n",
    "    r2 = 1.0 - (sum_squared_error / total_sum_squares) if total_sum_squares != 0 else 0.0\n",
    "    \n",
    "    logger.info(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "# Baseline Models\n",
    "def train_baseline_models(train_data, val_data):\n",
    "    \"\"\"Train and evaluate baseline models: Global Mean and Bias Model\"\"\"\n",
    "    logger.info(\"Training baseline models...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Global mean model\n",
    "    global_mean = train_data.select(\"rating\").agg({\"rating\": \"avg\"}).collect()[0][0]\n",
    "    logger.info(f\"Global mean rating: {global_mean:.4f}\")\n",
    "    \n",
    "    # Add global mean prediction column to validation data\n",
    "    val_global_mean = val_data.withColumn(\"prediction\", lit(global_mean))\n",
    "    \n",
    "    # Evaluate global mean model\n",
    "    logger.info(\"Global Mean Model Performance:\")\n",
    "    global_mean_metrics = evaluate_spark_model(val_global_mean)\n",
    "    \n",
    "    # Bias model: Calculate user and item biases\n",
    "    user_means = train_data.groupBy(\"userId\").agg({\"rating\": \"avg\"}).withColumnRenamed(\"avg(rating)\", \"user_mean\")\n",
    "    item_means = train_data.groupBy(\"movieId\").agg({\"rating\": \"avg\"}).withColumnRenamed(\"avg(rating)\", \"item_mean\")\n",
    "    \n",
    "    # Calculate biases (differences from global mean)\n",
    "    user_biases = user_means.withColumn(\"user_bias\", col(\"user_mean\") - global_mean)\n",
    "    item_biases = item_means.withColumn(\"item_bias\", col(\"item_mean\") - global_mean)\n",
    "    \n",
    "    # Add predictions to validation data\n",
    "    val_with_user = val_data.join(user_biases, \"userId\", \"left\")\n",
    "    val_with_user_item = val_with_user.join(item_biases, \"movieId\", \"left\")\n",
    "    \n",
    "    # Fill missing biases with 0\n",
    "    val_with_user_item = val_with_user_item.na.fill({\n",
    "        \"user_bias\": 0.0,\n",
    "        \"item_bias\": 0.0\n",
    "    })\n",
    "    \n",
    "    # Calculate bias prediction\n",
    "    val_with_predictions = val_with_user_item.withColumn(\n",
    "        \"prediction\", \n",
    "        global_mean + col(\"user_bias\") + col(\"item_bias\")\n",
    "    )\n",
    "    \n",
    "    # Evaluate bias model\n",
    "    logger.info(\"Bias Model Performance:\")\n",
    "    bias_model_metrics = evaluate_spark_model(val_with_predictions)\n",
    "    \n",
    "    logger.info(f\"Baseline models trained and evaluated in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract bias parameters for saving\n",
    "    user_biases_dict = {row['userId']: row['user_bias'] for row in user_biases.collect()}\n",
    "    item_biases_dict = {row['movieId']: row['item_bias'] for row in item_biases.collect()}\n",
    "    \n",
    "    # Create model parameters for later use\n",
    "    global_mean_params = {\n",
    "        'name': 'GlobalMean',\n",
    "        'global_mean': global_mean\n",
    "    }\n",
    "    \n",
    "    bias_model_params = {\n",
    "        'name': 'BiasModel',\n",
    "        'global_mean': global_mean,\n",
    "        'user_biases': user_biases_dict,\n",
    "        'item_biases': item_biases_dict\n",
    "    }\n",
    "    \n",
    "    return global_mean_params, bias_model_params, global_mean_metrics, bias_model_metrics\n",
    "\n",
    "\n",
    "def train_item_similarity_model_with_lsh(spark, train_data, val_data, n_neighbors=50, hash_tables=10, hash_length=4):\n",
    "    \"\"\"Train an item-based collaborative filtering model using Spark and LSH for efficient similarity search\"\"\"\n",
    "    logger.info(f\"Training item-based CF model with LSH (hash_tables={hash_tables}, hash_length={hash_length})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Normalize ratings by subtracting user means\n",
    "    user_means = train_data.groupBy(\"userId\").agg({\"rating\": \"avg\"}).withColumnRenamed(\"avg(rating)\", \"user_mean\")\n",
    "    train_normalized = train_data.join(user_means, \"userId\") \\\n",
    "                              .withColumn(\"normalized_rating\", col(\"rating\") - col(\"user_mean\"))\n",
    "    \n",
    "    # Create a matrix where rows are movies and columns are users\n",
    "    # First, pivot the data to create a sparse user-item matrix\n",
    "    # Get all unique users we'll need for the pivot\n",
    "    unique_users = train_normalized.select(\"userId\").distinct()\n",
    "    unique_users_list = [row.userId for row in unique_users.collect()]\n",
    "    \n",
    "    # For very large datasets, consider limiting to most active users\n",
    "    if len(unique_users_list) > 10000:\n",
    "        # Get user activity counts\n",
    "        user_counts = train_normalized.groupBy(\"userId\").count()\n",
    "        # Take top 10,000 most active users\n",
    "        active_users = user_counts.orderBy(col(\"count\").desc()).limit(10000)\n",
    "        active_user_ids = [row.userId for row in active_users.collect()]\n",
    "        # Filter to only include these users\n",
    "        train_normalized = train_normalized.filter(col(\"userId\").isin(active_user_ids))\n",
    "        unique_users_list = active_user_ids\n",
    "        logger.info(f\"Limited to top 10,000 most active users for LSH processing\")\n",
    "    \n",
    "    # Create expression for pivot\n",
    "    pivot_expr = {}\n",
    "    for user_id in unique_users_list:\n",
    "        pivot_expr[f\"user_{user_id}\"] = f\"sum(CASE WHEN userId = {user_id} THEN normalized_rating ELSE 0 END)\"\n",
    "    \n",
    "    # Create pivot query (note: this is a string-based approach for flexibility with many columns)\n",
    "    select_exprs = [f\"{expr} as {col_name}\" for col_name, expr in pivot_expr.items()]\n",
    "    pivot_query = f\"\"\"\n",
    "    SELECT movieId, {\", \".join(select_exprs)}\n",
    "    FROM train_normalized\n",
    "    GROUP BY movieId\n",
    "    \"\"\"\n",
    "    \n",
    "    # Register temp view for SQL query\n",
    "    train_normalized.createOrReplaceTempView(\"train_normalized\")\n",
    "    \n",
    "    # Execute pivot\n",
    "    logger.info(\"Creating pivoted item-user matrix...\")\n",
    "    item_features_df = spark.sql(pivot_query)\n",
    "    \n",
    "    # Convert to feature vectors for LSH\n",
    "    # Create vector assembler\n",
    "    vector_cols = [f\"user_{user_id}\" for user_id in unique_users_list]\n",
    "    assembler = VectorAssembler(inputCols=vector_cols, outputCol=\"features\")\n",
    "    item_vectors = assembler.transform(item_features_df)\n",
    "    \n",
    "    # Apply LSH\n",
    "    logger.info(\"Applying LSH for similarity search...\")\n",
    "    brp = BucketedRandomProjectionLSH(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"hashes\", \n",
    "        numHashTables=hash_tables,\n",
    "        bucketLength=hash_length\n",
    "    )\n",
    "    model = brp.fit(item_vectors)\n",
    "    item_hashed = model.transform(item_vectors)\n",
    "    \n",
    "    # Find approximate nearest neighbors for each item\n",
    "    logger.info(\"Finding approximate nearest neighbors...\")\n",
    "    \n",
    "    # Store neighbors\n",
    "    item_neighbors = {}\n",
    "    \n",
    "    # Process items in batches to avoid OOM errors\n",
    "    item_ids = [row.movieId for row in item_vectors.select(\"movieId\").collect()]\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(item_ids), batch_size):\n",
    "        batch_ids = item_ids[i:i+batch_size]\n",
    "        logger.info(f\"Processing batch {i//batch_size + 1}/{(len(item_ids)-1)//batch_size + 1}...\")\n",
    "        \n",
    "        for movie_id in batch_ids:\n",
    "            # Get the item vector\n",
    "            item = item_vectors.filter(col(\"movieId\") == movie_id)\n",
    "            \n",
    "            # Skip if item not found\n",
    "            if item.count() == 0:\n",
    "                continue\n",
    "                \n",
    "            # Find approximate nearest neighbors\n",
    "            distances = model.approxSimilarityJoin(\n",
    "                item, \n",
    "                item_hashed, \n",
    "                threshold=2.0,  # Adjust threshold as needed\n",
    "                distCol=\"distance\"\n",
    "            )\n",
    "            \n",
    "            # Filter out self-matches and convert distance to similarity\n",
    "            neighbors = distances.filter(\n",
    "                (col(\"datasetA.movieId\") == movie_id) & \n",
    "                (col(\"datasetB.movieId\") != movie_id)\n",
    "            ).select(\n",
    "                col(\"datasetB.movieId\").alias(\"neighbor_id\"),\n",
    "                (1.0 / (1.0 + col(\"distance\"))).alias(\"similarity\")  # Convert distance to similarity\n",
    "            ).orderBy(desc(\"similarity\")).limit(n_neighbors)\n",
    "            \n",
    "            # Collect and store neighbors\n",
    "            item_neighbors[movie_id] = [\n",
    "                (row.neighbor_id, row.similarity) \n",
    "                for row in neighbors.collect()\n",
    "            ]\n",
    "    \n",
    "    # Create user means dictionary\n",
    "    user_means_dict = {row.userId: row.user_mean for row in user_means.collect()}\n",
    "    \n",
    "    # Create model parameters\n",
    "    item_cf_params = {\n",
    "        'name': 'ItemCF_LSH',\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'user_means': user_means_dict,\n",
    "        'item_neighbors': item_neighbors\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Item-based CF model with LSH completed in {time.time() - start_time:.2f} seconds\")\n",
    "    logger.info(f\"Generated neighborhoods for {len(item_neighbors)} items\")\n",
    "    \n",
    "    return item_cf_params\n",
    "\n",
    "\n",
    "def generate_item_cf_recommendations(item_cf_params, user_id, user_ratings, movies_df, n=10):\n",
    "    \"\"\"Generate recommendations using item-based CF model\"\"\"\n",
    "    # Check if user exists in model\n",
    "    if user_id not in item_cf_params['user_means']:\n",
    "        logger.warning(f\"User {user_id} not found in model, using global average\")\n",
    "        # Return popular recommendations instead\n",
    "        return []\n",
    "        \n",
    "    # Get user mean\n",
    "    user_mean = item_cf_params['user_means'].get(user_id, 0.0)\n",
    "    \n",
    "    # Get items rated by the user\n",
    "    user_rated_items = set()\n",
    "    user_item_ratings = {}\n",
    "    \n",
    "    # Extract user's ratings\n",
    "    for row in user_ratings.filter(col(\"userId\") == user_id).collect():\n",
    "        movie_id = row.movieId\n",
    "        rating = row.rating\n",
    "        user_rated_items.add(movie_id)\n",
    "        user_item_ratings[movie_id] = rating\n",
    "    \n",
    "    # Compute predicted scores for candidate items\n",
    "    candidate_scores = {}\n",
    "    \n",
    "    # For each item the user rated\n",
    "    for item_id, rating in user_item_ratings.items():\n",
    "        # Normalize the rating\n",
    "        normalized_rating = rating - user_mean\n",
    "        \n",
    "        # Get similar items\n",
    "        if item_id in item_cf_params['item_neighbors']:\n",
    "            similar_items = item_cf_params['item_neighbors'][item_id]\n",
    "            \n",
    "            for similar_id, similarity in similar_items:\n",
    "                # Skip items the user has already rated\n",
    "                if similar_id in user_rated_items:\n",
    "                    continue\n",
    "                    \n",
    "                # Update the candidate score\n",
    "                if similar_id not in candidate_scores:\n",
    "                    candidate_scores[similar_id] = {'sim_sum': 0.0, 'weighted_sum': 0.0}\n",
    "                    \n",
    "                candidate_scores[similar_id]['sim_sum'] += similarity\n",
    "                candidate_scores[similar_id]['weighted_sum'] += similarity * normalized_rating\n",
    "    \n",
    "    # Compute final predictions\n",
    "    predictions = []\n",
    "    for movie_id, scores in candidate_scores.items():\n",
    "        if scores['sim_sum'] > 0:\n",
    "            predicted_rating = user_mean + (scores['weighted_sum'] / scores['sim_sum'])\n",
    "            \n",
    "            # Clip predictions to valid range\n",
    "            predicted_rating = max(0.5, min(5.0, predicted_rating))\n",
    "            \n",
    "            predictions.append((movie_id, predicted_rating))\n",
    "    \n",
    "    # Sort by predicted rating and get top N\n",
    "    top_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Get movie details\n",
    "    recommended_movies = []\n",
    "    for movie_id, score in top_predictions:\n",
    "        movie_info = movies_df.filter(col(\"movieId\") == movie_id).collect()\n",
    "        if movie_info:\n",
    "            movie = movie_info[0]\n",
    "            recommended_movies.append({\n",
    "                \"movieId\": movie_id,\n",
    "                \"title\": movie[\"title\"],\n",
    "                \"genres\": movie[\"genres\"],\n",
    "                \"predicted_rating\": score\n",
    "            })\n",
    "    \n",
    "    return recommended_movies\n",
    "\n",
    "def evaluate_lsh_model(item_cf_params, val_data, movies_df):\n",
    "    \"\"\"Evaluate the LSH-based item similarity model\"\"\"\n",
    "    logger.info(\"Evaluating LSH-based item similarity model...\")\n",
    "    \n",
    "    # Sample users for evaluation (for speed)\n",
    "    user_sample = val_data.select(\"userId\").distinct().sample(fraction=0.1).collect()\n",
    "    sample_user_ids = [row.userId for row in user_sample]\n",
    "    \n",
    "    # Filter validation data to these users\n",
    "    val_sample = val_data.filter(col(\"userId\").isin(sample_user_ids))\n",
    "    \n",
    "    # Get all ratings for these users (including training data)\n",
    "    user_ratings = spark.sql(\"SELECT * FROM train_normalized\")\n",
    "    \n",
    "    # Track metrics\n",
    "    prediction_count = 0\n",
    "    rmse_sum = 0.0\n",
    "    mae_sum = 0.0\n",
    "    \n",
    "    # Process each user\n",
    "    for user_id in sample_user_ids:\n",
    "        # Get user's validation ratings\n",
    "        user_val_ratings = val_sample.filter(col(\"userId\") == user_id).collect()\n",
    "        \n",
    "        if not user_val_ratings:\n",
    "            continue\n",
    "            \n",
    "        # Generate recommendations for all possible movies\n",
    "        all_recs = generate_item_cf_recommendations(\n",
    "            item_cf_params, user_id, user_ratings, movies_df, n=1000\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary of predicted ratings\n",
    "        predictions = {rec[\"movieId\"]: rec[\"predicted_rating\"] for rec in all_recs}\n",
    "        \n",
    "        # Compare with actual ratings\n",
    "        for row in user_val_ratings:\n",
    "            movie_id = row.movieId\n",
    "            actual_rating = row.rating\n",
    "            \n",
    "            if movie_id in predictions:\n",
    "                pred_rating = predictions[movie_id]\n",
    "                \n",
    "                # Update metrics\n",
    "                rmse_sum += (pred_rating - actual_rating) ** 2\n",
    "                mae_sum += abs(pred_rating - actual_rating)\n",
    "                prediction_count += 1\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    if prediction_count > 0:\n",
    "        rmse = math.sqrt(rmse_sum / prediction_count)\n",
    "        mae = mae_sum / prediction_count\n",
    "        \n",
    "        logger.info(f\"LSH Model Evaluation - RMSE: {rmse:.4f}, MAE: {mae:.4f}, Count: {prediction_count}\")\n",
    "        \n",
    "        return {\"RMSE\": rmse, \"MAE\": mae}\n",
    "    else:\n",
    "        logger.warning(\"No predictions could be generated for evaluation\")\n",
    "        return {\"RMSE\": float('inf'), \"MAE\": float('inf')}\n",
    "\n",
    "\n",
    "\n",
    "def save_lsh_model(item_cf_params, output_dir=\"streamlit_models\"):\n",
    "    \"\"\"Save LSH model parameters efficiently\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    logger.info(f\"Saving LSH model parameters to {output_dir}...\")\n",
    "    \n",
    "    # Save in chunks to avoid memory issues\n",
    "    # 1. Save user means\n",
    "    with open(os.path.join(output_dir, \"item_cf_user_means.pkl\"), 'wb') as f:\n",
    "        pickle.dump(item_cf_params['user_means'], f)\n",
    "    \n",
    "    # 2. Save item neighbors in batches\n",
    "    item_ids = list(item_cf_params['item_neighbors'].keys())\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for i in range(0, len(item_ids), batch_size):\n",
    "        batch_ids = item_ids[i:i+batch_size]\n",
    "        batch_neighbors = {item_id: item_cf_params['item_neighbors'][item_id] \n",
    "                          for item_id in batch_ids}\n",
    "        \n",
    "        with open(os.path.join(output_dir, f\"item_cf_neighbors_batch_{i//batch_size}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(batch_neighbors, f)\n",
    "    \n",
    "    # 3. Save metadata\n",
    "    metadata = {\n",
    "        'name': item_cf_params['name'],\n",
    "        'n_neighbors': item_cf_params['n_neighbors'],\n",
    "        'num_items': len(item_ids),\n",
    "        'num_users': len(item_cf_params['user_means']),\n",
    "        'num_batches': (len(item_ids) - 1) // batch_size + 1\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"item_cf_metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"LSH model saved in {(len(item_ids) - 1) // batch_size + 1} batches\")\n",
    "\n",
    "# ALS model training\n",
    "def train_als_model(train_data, val_data, rank=100, regParam=0.1, maxIter=15, seed=42, implicit=False):\n",
    "    \"\"\"Train an ALS model with specified parameters\"\"\"\n",
    "    logger.info(f\"Training ALS model (rank={rank}, regParam={regParam}, maxIter={maxIter}, implicit={implicit})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create an ALS model\n",
    "    als = ALS(\n",
    "        rank=rank,\n",
    "        maxIter=maxIter,\n",
    "        regParam=regParam,\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        seed=seed,\n",
    "        nonnegative=True,  # Enforce non-negative factors\n",
    "        implicitPrefs=implicit  # Set to True for implicit feedback\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model = als.fit(train_data)\n",
    "    \n",
    "    # Make predictions on validation data\n",
    "    predictions = model.transform(val_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    logger.info(\"ALS Model Performance:\")\n",
    "    als_metrics = evaluate_spark_model(predictions)\n",
    "    \n",
    "    logger.info(f\"ALS model trained in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return model, als_metrics\n",
    "\n",
    "# Function to tune ALS parameters\n",
    "def tune_als_model(train_data, val_data, ranks=[10, 50, 100], regParams=[0.01, 0.1, 1.0], maxIters=[10, 15]):\n",
    "    \"\"\"Find the best ALS parameters through grid search\"\"\"\n",
    "    logger.info(\"Tuning ALS model parameters...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Grid search\n",
    "    for rank in ranks:\n",
    "        for regParam in regParams:\n",
    "            for maxIter in maxIters:\n",
    "                logger.info(f\"Trying rank={rank}, regParam={regParam}, maxIter={maxIter}\")\n",
    "                \n",
    "                # Train model with current parameters\n",
    "                model, metrics = train_als_model(\n",
    "                    train_data, val_data, \n",
    "                    rank=rank, regParam=regParam, maxIter=maxIter\n",
    "                )\n",
    "                \n",
    "                rmse = metrics['RMSE']\n",
    "                \n",
    "                results.append({\n",
    "                    'rank': rank,\n",
    "                    'regParam': regParam,\n",
    "                    'maxIter': maxIter,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': metrics['MAE'],\n",
    "                    'R2': metrics['R2']\n",
    "                })\n",
    "                \n",
    "                # Check if this model is better\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_params = (rank, regParam, maxIter)\n",
    "                    best_model = model\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print best parameters\n",
    "    best_rank, best_regParam, best_maxIter = best_params\n",
    "    logger.info(f\"Best parameters: rank={best_rank}, regParam={best_regParam}, maxIter={best_maxIter}\")\n",
    "    logger.info(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "    logger.info(f\"ALS parameter tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save tuning results\n",
    "    results_df.to_csv(\"als_tuning_results.csv\", index=False)\n",
    "    \n",
    "    # Return best model and results\n",
    "    return best_model, results_df\n",
    "\n",
    "# Extract ALS model parameters for Streamlit\n",
    "def extract_als_parameters(als_model, original_params=None):\n",
    "    \"\"\"Extract user and item factors from ALS model for use in Streamlit\"\"\"\n",
    "    logger.info(\"Extracting ALS model parameters...\")\n",
    "    \n",
    "    # Extract user factors\n",
    "    user_factors = als_model.userFactors.collect()\n",
    "    user_factor_dict = {row.id: row.features for row in user_factors}\n",
    "    \n",
    "    # Extract item factors\n",
    "    item_factors = als_model.itemFactors.collect()\n",
    "    item_factor_dict = {row.id: row.features for row in item_factors}\n",
    "    \n",
    "    # Create model parameters\n",
    "    als_params = {\n",
    "        'name': 'ALS',\n",
    "        'rank': als_model.rank,\n",
    "        'user_factors': user_factor_dict,\n",
    "        'item_factors': item_factor_dict\n",
    "    }\n",
    "    \n",
    "    # Add original parameters if provided\n",
    "    if original_params:\n",
    "        als_params.update({\n",
    "            'maxIter': original_params.get('maxIter', 10),\n",
    "            'regParam': original_params.get('regParam', 0.1),\n",
    "            'nonnegative': original_params.get('nonnegative', True),\n",
    "            'implicitPrefs': original_params.get('implicitPrefs', False)\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Extracted factors for {len(user_factor_dict)} users and {len(item_factor_dict)} items\")\n",
    "    \n",
    "    return als_params\n",
    "\n",
    "# Content-based filtering using genre information\n",
    "def train_content_based_model(spark, movies_df, train_data, all_genres):\n",
    "    \"\"\"Train a content-based filtering model using movie genres\"\"\"\n",
    "    logger.info(\"Training content-based filtering model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate user genre preferences based on their ratings\n",
    "    # For each user, calculate average rating for each genre\n",
    "    \n",
    "    # First, get the genre columns\n",
    "    genre_columns = [f\"genre_{genre}\" for genre in all_genres]\n",
    "    \n",
    "    # Join ratings with movies to get genre information\n",
    "    user_ratings_with_genres = train_data.join(movies_df.select(\"movieId\", *genre_columns), \"movieId\")\n",
    "    \n",
    "    # Create user profiles by calculating average rating per genre\n",
    "    user_profiles = {}\n",
    "    \n",
    "    # Process user ratings in batches to create profiles\n",
    "    for user_row in user_ratings_with_genres.groupBy(\"userId\").count().collect():\n",
    "        user_id = user_row[\"userId\"]\n",
    "        \n",
    "        # Get this user's ratings\n",
    "        user_ratings = user_ratings_with_genres.filter(col(\"userId\") == user_id)\n",
    "        \n",
    "        # Calculate average rating per genre\n",
    "        genre_ratings = []\n",
    "        genre_counts = []\n",
    "        \n",
    "        # Sum ratings and counts for each genre\n",
    "        for genre in all_genres:\n",
    "            genre_col = f\"genre_{genre}\"\n",
    "            genre_data = user_ratings.filter(col(genre_col) == 1)\n",
    "            \n",
    "            if genre_data.count() > 0:\n",
    "                avg_rating = genre_data.agg({\"rating\": \"avg\"}).collect()[0][0]\n",
    "                genre_ratings.append(avg_rating)\n",
    "                genre_counts.append(genre_data.count())\n",
    "            else:\n",
    "                genre_ratings.append(0.0)\n",
    "                genre_counts.append(0)\n",
    "        \n",
    "        # Create normalized profile\n",
    "        total = sum([r * c for r, c in zip(genre_ratings, genre_counts) if c > 0])\n",
    "        if total > 0:\n",
    "            profile = [(r * c) / total if c > 0 else 0.0 for r, c in zip(genre_ratings, genre_counts)]\n",
    "            # Normalize to unit length\n",
    "            norm = math.sqrt(sum([p*p for p in profile]))\n",
    "            if norm > 0:\n",
    "                profile = [p/norm for p in profile]\n",
    "            user_profiles[user_id] = profile\n",
    "    \n",
    "    # Create genre vectors for movies\n",
    "    movie_genre_vectors = {}\n",
    "    \n",
    "    # Process each movie\n",
    "    for movie_row in movies_df.select(\"movieId\", *genre_columns).collect():\n",
    "        movie_id = movie_row[\"movieId\"]\n",
    "        genre_vector = [float(movie_row[genre_col]) for genre_col in genre_columns]\n",
    "        \n",
    "        # Normalize vector\n",
    "        norm = math.sqrt(sum([g*g for g in genre_vector]))\n",
    "        if norm > 0:\n",
    "            genre_vector = [g/norm for g in genre_vector]\n",
    "        \n",
    "        movie_genre_vectors[movie_id] = genre_vector\n",
    "    \n",
    "    # Create content model parameters\n",
    "    content_params = {\n",
    "        'name': 'ContentBased',\n",
    "        'user_profiles': user_profiles,\n",
    "        'movie_genre_vectors': movie_genre_vectors,\n",
    "        'genres': all_genres\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Content-based model trained in {time.time() - start_time:.2f} seconds\")\n",
    "    logger.info(f\"Created profiles for {len(user_profiles)} users\")\n",
    "    \n",
    "    return content_params\n",
    "\n",
    "# Hybrid model - combine ALS and bias model\n",
    "def create_hybrid_model(bias_model_params, als_model, val_data, weight_als=0.7):\n",
    "    \"\"\"Create a hybrid model combining baseline bias and ALS predictions\"\"\"\n",
    "    logger.info(f\"Creating hybrid model with ALS weight={weight_als}\")\n",
    "    \n",
    "    # Get bias model parameters\n",
    "    global_mean = bias_model_params['global_mean']\n",
    "    user_biases = bias_model_params['user_biases']\n",
    "    item_biases = bias_model_params['item_biases']\n",
    "    \n",
    "    # Create bias prediction function\n",
    "    def bias_prediction(user_id, movie_id):\n",
    "        user_bias = user_biases.get(user_id, 0.0)\n",
    "        item_bias = item_biases.get(movie_id, 0.0)\n",
    "        return global_mean + user_bias + item_bias\n",
    "    \n",
    "    # Register UDF\n",
    "    bias_prediction_udf = udf(bias_prediction, FloatType())\n",
    "    \n",
    "    # Add bias model predictions to validation data\n",
    "    val_with_bias = val_data.withColumn(\n",
    "        \"bias_prediction\",\n",
    "        bias_prediction_udf(col(\"userId\"), col(\"movieId\"))\n",
    "    )\n",
    "    \n",
    "    # Add ALS predictions\n",
    "    val_with_hybrid = als_model.transform(val_with_bias)\n",
    "    \n",
    "    # Create hybrid prediction\n",
    "    val_with_hybrid = val_with_hybrid.withColumn(\n",
    "        \"hybrid_prediction\",\n",
    "        (1 - weight_als) * col(\"bias_prediction\") + weight_als * col(\"prediction\")\n",
    "    )\n",
    "    \n",
    "    # Evaluate hybrid model\n",
    "    logger.info(\"Hybrid Model Performance:\")\n",
    "    hybrid_metrics = evaluate_spark_model(\n",
    "        val_with_hybrid, \n",
    "        truth_col=\"rating\", \n",
    "        pred_col=\"hybrid_prediction\"\n",
    "    )\n",
    "    \n",
    "    # Create hybrid model parameters\n",
    "    hybrid_params = {\n",
    "        'name': 'HybridModel',\n",
    "        'bias_model': bias_model_params,\n",
    "        'als_model_name': 'ALS',  # Reference to ALS model\n",
    "        'weight_als': weight_als\n",
    "    }\n",
    "    \n",
    "    return hybrid_params, hybrid_metrics\n",
    "\n",
    "# Create popularity-based recommendations\n",
    "def create_popularity_recommendations(train_data, movies_df, all_genres, n=100):\n",
    "    \"\"\"Create popularity-based recommendation lists\"\"\"\n",
    "    logger.info(\"Creating popularity-based recommendation lists...\")\n",
    "    \n",
    "    # Calculate popularity metrics\n",
    "    pop_metrics = train_data.groupBy(\"movieId\").agg(\n",
    "        {\"rating\": \"count\", \"rating\": \"avg\"}\n",
    "    ).withColumnRenamed(\"count(rating)\", \"rating_count\") \\\n",
    "     .withColumnRenamed(\"avg(rating)\", \"avg_rating\")\n",
    "    \n",
    "    # Join with movie data\n",
    "    pop_with_info = pop_metrics.join(movies_df.select(\"movieId\", \"title\", \"genres\"), \"movieId\")\n",
    "    \n",
    "    # Calculate minimum interactions for consideration (median)\n",
    "    min_interactions = pop_metrics.approxQuantile(\"rating_count\", [0.5], 0.1)[0]\n",
    "    logger.info(f\"Minimum interactions threshold: {min_interactions}\")\n",
    "    \n",
    "    # Create different recommendation lists\n",
    "    \n",
    "    # 1. Most Popular (by number of ratings)\n",
    "    most_popular = pop_with_info.filter(col(\"rating_count\") >= min_interactions) \\\n",
    "                                .orderBy(col(\"rating_count\").desc())\n",
    "    \n",
    "    # 2. Highest Rated (with minimum interactions)\n",
    "    highest_rated = pop_with_info.filter(col(\"rating_count\") >= min_interactions) \\\n",
    "                                 .orderBy(col(\"avg_rating\").desc())\n",
    "    \n",
    "    # Convert to small pandas dataframes for storage\n",
    "    most_popular_df = most_popular.select(\"movieId\", \"title\", \"genres\", \"rating_count\", \"avg_rating\") \\\n",
    "                                 .limit(n).toPandas()\n",
    "    \n",
    "    highest_rated_df = highest_rated.select(\"movieId\", \"title\", \"genres\", \"rating_count\", \"avg_rating\") \\\n",
    "                                   .limit(n).toPandas()\n",
    "    \n",
    "    # Create genre-based popularity lists\n",
    "    genre_lists = {}\n",
    "    \n",
    "    # Get popular movies by genre\n",
    "    for genre in all_genres:\n",
    "        genre_popular = pop_with_info.join(\n",
    "            movies_df.select(\"movieId\", f\"genre_{genre}\"), \n",
    "            \"movieId\"\n",
    "        ).filter(col(f\"genre_{genre}\") == 1) \\\n",
    "         .filter(col(\"rating_count\") >= min_interactions / 2) \\\n",
    "         .orderBy(col(\"rating_count\").desc()) \\\n",
    "         .select(\"movieId\", \"title\", \"genres\", \"rating_count\", \"avg_rating\") \\\n",
    "         .limit(20)\n",
    "        \n",
    "        genre_lists[genre] = genre_popular.toPandas()\n",
    "    \n",
    "    # Create popularity model parameters\n",
    "    popularity_params = {\n",
    "        'name': 'Popularity',\n",
    "        'most_popular': most_popular_df,\n",
    "        'highest_rated': highest_rated_df,\n",
    "        'genre_lists': genre_lists\n",
    "    }\n",
    "    \n",
    "    return popularity_params\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate recommendations using ALS model parameters\n",
    "def generate_als_recommendations(als_params, user_id, movies_df, n=10):\n",
    "    \"\"\"Generate movie recommendations using ALS model parameters\"\"\"\n",
    "    # Get user factors\n",
    "    if user_id not in als_params['user_factors']:\n",
    "        logger.warning(f\"No factors found for user {user_id}\")\n",
    "        return []\n",
    "    \n",
    "    user_factor = als_params['user_factors'][user_id]\n",
    "    \n",
    "    # Calculate ratings for all items\n",
    "    item_scores = []\n",
    "    for movie_id, item_factor in als_params['item_factors'].items():\n",
    "        # Dot product between user and item factors\n",
    "        score = sum(u * i for u, i in zip(user_factor, item_factor))\n",
    "        item_scores.append((movie_id, score))\n",
    "    \n",
    "    # Sort by score and get top N\n",
    "    top_items = sorted(item_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Get movie details\n",
    "    recommendations = []\n",
    "    for movie_id, score in top_items:\n",
    "        movie_info = movies_df.filter(col(\"movieId\") == movie_id).collect()\n",
    "        if movie_info:\n",
    "            movie = movie_info[0]\n",
    "            recommendations.append({\n",
    "                \"movieId\": movie_id,\n",
    "                \"title\": movie[\"title\"],\n",
    "                \"genres\": movie[\"genres\"],\n",
    "                \"predicted_rating\": score\n",
    "            })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate content-based recommendations\n",
    "def generate_content_recommendations(content_params, user_id, movies_df, n=10):\n",
    "    \"\"\"Generate recommendations based on content similarity\"\"\"\n",
    "    # Get user profile\n",
    "    if user_id not in content_params['user_profiles']:\n",
    "        logger.warning(f\"No profile found for user {user_id}\")\n",
    "        return []\n",
    "    \n",
    "    user_profile = content_params['user_profiles'][user_id]\n",
    "    \n",
    "    # Calculate similarity with all movies\n",
    "    movie_scores = []\n",
    "    for movie_id, genre_vector in content_params['movie_genre_vectors'].items():\n",
    "        # Compute cosine similarity\n",
    "        similarity = sum(u * i for u, i in zip(user_profile, genre_vector))\n",
    "        movie_scores.append((movie_id, similarity))\n",
    "    \n",
    "    # Sort by similarity and get top N\n",
    "    top_items = sorted(movie_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Get movie details\n",
    "    recommendations = []\n",
    "    for movie_id, similarity in top_items:\n",
    "        movie_info = movies_df.filter(col(\"movieId\") == movie_id).collect()\n",
    "        if movie_info:\n",
    "            movie = movie_info[0]\n",
    "            recommendations.append({\n",
    "                \"movieId\": movie_id,\n",
    "                \"title\": movie[\"title\"],\n",
    "                \"genres\": movie[\"genres\"],\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate hybrid recommendations\n",
    "def generate_hybrid_recommendations(hybrid_params, als_params, user_id, movies_df, n=10):\n",
    "    \"\"\"Generate recommendations using hybrid model parameters\"\"\"\n",
    "    # Get predictions from ALS model\n",
    "    als_recs = generate_als_recommendations(als_params, user_id, movies_df, n=n*2)\n",
    "    \n",
    "    # Get bias model parameters\n",
    "    bias_model = hybrid_params['bias_model']\n",
    "    global_mean = bias_model['global_mean']\n",
    "    user_biases = bias_model['user_biases']\n",
    "    item_biases = bias_model['item_biases']\n",
    "    \n",
    "    # Apply hybrid weighting\n",
    "    weight_als = hybrid_params['weight_als']\n",
    "    weight_bias = 1.0 - weight_als\n",
    "    \n",
    "    hybrid_scores = []\n",
    "    for rec in als_recs:\n",
    "        movie_id = rec[\"movieId\"]\n",
    "        als_score = rec[\"predicted_rating\"]\n",
    "        \n",
    "        # Calculate bias score\n",
    "        user_bias = user_biases.get(user_id, 0.0)\n",
    "        item_bias = item_biases.get(movie_id, 0.0)\n",
    "        bias_score = global_mean + user_bias + item_bias\n",
    "        \n",
    "        # Calculate hybrid score\n",
    "        hybrid_score = weight_als * als_score + weight_bias * bias_score\n",
    "        \n",
    "        hybrid_scores.append((movie_id, hybrid_score, rec[\"title\"], rec[\"genres\"]))\n",
    "    \n",
    "    # Sort by hybrid score and get top N\n",
    "    top_items = sorted(hybrid_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Format recommendations\n",
    "    recommendations = []\n",
    "    for movie_id, score, title, genres in top_items:\n",
    "        recommendations.append({\n",
    "            \"movieId\": movie_id,\n",
    "            \"title\": title,\n",
    "            \"genres\": genres,\n",
    "            \"predicted_rating\": score\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Save models for Streamlit app\n",
    "def save_models_for_streamlit(model_params, output_dir=\"streamlit_models\"):\n",
    "    \"\"\"Save model parameters in pickle format for Streamlit\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    logger.info(f\"Saving model parameters to {output_dir}...\")\n",
    "    \n",
    "    # Save all model parameters\n",
    "    for model_name, params in model_params.items():\n",
    "        model_file = os.path.join(output_dir, f\"{model_name}_params.pkl\")\n",
    "        \n",
    "        # For large model parameters, consider saving in chunks or processing\n",
    "        # before saving to reduce memory footprint\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        \n",
    "        logger.info(f\"Saved {model_name} parameters to {model_file}\")\n",
    "    \n",
    "    # Create model info file\n",
    "    model_info = {\n",
    "        'available_models': list(model_params.keys()),\n",
    "        'saved_date': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"model_info.json\"), 'w') as f:\n",
    "        json.dump(model_info, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"All model parameters saved successfully to {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation visualization\n",
    "def create_evaluation_visualizations(metrics_dict, output_path=\"model_evaluation.png\"):\n",
    "    \"\"\"Create visualizations of model performance\"\"\"\n",
    "    # Convert metrics dictionary to DataFrame\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index')\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot RMSE\n",
    "    plt.subplot(2, 1, 1)\n",
    "    metrics_df['RMSE'].plot(kind='bar', color='skyblue')\n",
    "    plt.title('Model Comparison - RMSE (lower is better)')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(2, 1, 2)\n",
    "    metrics_df['MAE'].plot(kind='bar', color='lightgreen')\n",
    "    plt.title('Model Comparison - MAE (lower is better)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    logger.info(f\"Model comparison visualization saved to {output_path}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark\n",
    "    spark = init_spark(memory=\"8g\")  # Adjust memory based on your system\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare data\n",
    "        ratings_df, movies_df, n_users, n_movies, active_users, active_movies = load_data(\n",
    "            spark, \"ml-32m\", min_ratings=5\n",
    "        )\n",
    "        \n",
    "        # Process movie features\n",
    "        movies_processed_df, all_genres = process_movie_features(spark, movies_df)\n",
    "        \n",
    "        # Split data\n",
    "        train_data, val_data, test_data = train_val_test_split(ratings_df)\n",
    "        \n",
    "        # Train baseline models\n",
    "        global_mean_params, bias_model_params, global_mean_metrics, bias_model_metrics = train_baseline_models(\n",
    "            train_data, val_data\n",
    "        )\n",
    "        \n",
    "        # Train ALS model with updated parameter extraction\n",
    "        rank, regParam, maxIter = 50, 0.1, 10\n",
    "        original_params = {\n",
    "            'rank': rank,\n",
    "            'regParam': regParam,\n",
    "            'maxIter': maxIter,\n",
    "            'nonnegative': True,\n",
    "            'implicitPrefs': False\n",
    "        }\n",
    "        \n",
    "        als_model, als_metrics = train_als_model(\n",
    "            train_data, val_data, \n",
    "            rank=rank, regParam=regParam, maxIter=maxIter\n",
    "        )\n",
    "        \n",
    "        # Extract ALS parameters for Streamlit with original params\n",
    "        als_params = extract_als_parameters(als_model, original_params)\n",
    "        \n",
    "        # # Train item-based CF model with LSH\n",
    "        # item_cf_params = train_item_similarity_model_with_lsh(\n",
    "        #     spark, train_data, val_data, n_neighbors=30, hash_tables=10, hash_length=4.0\n",
    "        # )\n",
    "        \n",
    "        # # Evaluate LSH model\n",
    "        # lsh_metrics = evaluate_lsh_model(item_cf_params, val_data, movies_processed_df)\n",
    "        # Create hybrid model\n",
    "        hybrid_params, hybrid_metrics = create_hybrid_model(\n",
    "            bias_model_params, als_model, val_data, weight_als=0.7\n",
    "        )\n",
    "        \n",
    "        # Train content-based model\n",
    "        content_params = train_content_based_model(\n",
    "            spark, movies_processed_df, train_data, all_genres\n",
    "        )\n",
    "        \n",
    "        # Create popularity recommendations\n",
    "        popularity_params = create_popularity_recommendations(\n",
    "            train_data, movies_processed_df, all_genres, n=100\n",
    "        )\n",
    "\n",
    "        # Collect all model parameters\n",
    "        all_model_params = {\n",
    "            'global_mean': global_mean_params,\n",
    "            'bias': bias_model_params,\n",
    "            'als': als_params,\n",
    "            # 'item_cf': item_cf_params,\n",
    "            'hybrid': hybrid_params,\n",
    "            'content': content_params,\n",
    "            'popularity': popularity_params\n",
    "        }\n",
    "        # Collect all metrics\n",
    "        all_metrics = {\n",
    "            'Global Mean': global_mean_metrics,\n",
    "            'Bias Model': bias_model_metrics,\n",
    "            'ALS': als_metrics,\n",
    "            # 'Item-CF-LSH': lsh_metrics,\n",
    "            'Hybrid': hybrid_metrics\n",
    "        }\n",
    "        \n",
    "        # Create evaluation visualizations\n",
    "        create_evaluation_visualizations(all_metrics)\n",
    "        \n",
    "        # Save model parameters for Streamlit\n",
    "        save_models_for_streamlit(all_model_params)\n",
    "        \n",
    "        # Save LSH model separately due to size\n",
    "        # save_lsh_model(item_cf_params)\n",
    "        # Generate example recommendations\n",
    "        sample_user_id = active_users.first()['userId']\n",
    "        \n",
    "        logger.info(f\"\\nExample recommendations for user {sample_user_id}:\")\n",
    "        \n",
    "        # ALS recommendations\n",
    "        als_recs = generate_als_recommendations(als_params, sample_user_id, movies_processed_df, n=5)\n",
    "        logger.info(\"ALS Recommendations:\")\n",
    "        for i, rec in enumerate(als_recs):\n",
    "            logger.info(f\"{i+1}. {rec['title']} - Predicted rating: {rec['predicted_rating']:.2f}\")\n",
    "        \n",
    "        # Content-based recommendations\n",
    "        content_recs = generate_content_recommendations(content_params, sample_user_id, movies_processed_df, n=5)\n",
    "        logger.info(\"\\nContent-Based Recommendations:\")\n",
    "        for i, rec in enumerate(content_recs):\n",
    "            logger.info(f\"{i+1}. {rec['title']} - Similarity: {rec['similarity']:.4f}\")\n",
    "        \n",
    "        # Hybrid recommendations\n",
    "        hybrid_recs = generate_hybrid_recommendations(hybrid_params, als_params, sample_user_id, movies_processed_df, n=5)\n",
    "        logger.info(\"\\nHybrid Recommendations:\")\n",
    "        for i, rec in enumerate(hybrid_recs):\n",
    "            logger.info(f\"{i+1}. {rec['title']} - Predicted rating: {rec['predicted_rating']:.2f}\")\n",
    "        \n",
    "        logger.info(\"\\nRecommendation model pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in recommendation pipeline: {str(e)}\", exc_info=True)\n",
    "    finally:\n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "        logger.info(\"Spark session stopped\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Run main function\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sltd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
