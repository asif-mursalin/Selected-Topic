{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb53114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 06:56:23.625211: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Collaborative Filtering model training...\n",
      "Loading MovieLens 100K dataset...\n",
      "Dataset loaded. Training set size: 1476893, Test set size: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 06:56:25.817466: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2025-04-15 06:56:25.921443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [80000]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2025-04-15 06:56:25.921669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [80000]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully. Summary:\n",
      "Model: \"ncf_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup (StringLookup  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " user_embedding (Embedding)  multiple                  30208     \n",
      "                                                                 \n",
      " string_lookup_1 (StringLook  multiple                 0         \n",
      " up)                                                             \n",
      "                                                                 \n",
      " movie_embedding (Embedding)  multiple                 53856     \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,849\n",
      "Trainable params: 90,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 06:56:26.132715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [80000]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2025-04-15 06:56:26.132919: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [80000]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/625 [============================>.] - ETA: 0s - loss: 2.0109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 06:56:30.066171: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [20000]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 4s 5ms/step - loss: 2.0074 - val_loss: 0.9159\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.8949 - val_loss: 0.8977\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.8749 - val_loss: 0.8970\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.8537 - val_loss: 0.8657\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.8325 - val_loss: 0.8608\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.8165 - val_loss: 0.8661\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.7967 - val_loss: 0.8550\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.7754 - val_loss: 0.8505\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.7541 - val_loss: 0.8509\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.7323 - val_loss: 0.8594\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.7119 - val_loss: 0.8648\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.6911 - val_loss: 0.8678\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.6705 - val_loss: 0.8767\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.6476 - val_loss: 0.8920\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.6277 - val_loss: 0.9037\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.6062 - val_loss: 0.9146\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.5873 - val_loss: 0.9355\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.5664 - val_loss: 0.9494\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.5471 - val_loss: 0.9640\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.5281 - val_loss: 0.9760\n",
      "Training completed in 62.37 seconds\n",
      "\n",
      "Evaluating the model...\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.9760\n",
      "RMSE: 0.9879\n",
      "\n",
      "Saving the model...\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving model: this __dict__ descriptor does not support '_DictWrapper' objects\n",
      "Saved model weights to models/ncf_model_weights.h5\n",
      "Neural Collaborative Filtering model saved successfully\n",
      "\n",
      "Model training and saving complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_movielens_data():\n",
    "    \"\"\"Load the MovieLens dataset and prepare it for training\"\"\"\n",
    "    print(\"Loading MovieLens 100K dataset...\")\n",
    "    # Use the built-in movielens-100k dataset\n",
    "    data = Dataset.load_builtin('ml-100k')\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Dataset loaded. Training set size: {len(trainset.build_anti_testset())}, Test set size: {len(testset)}\")\n",
    "    \n",
    "    # Convert to format suitable for TensorFlow\n",
    "    raw_trainset = []\n",
    "    for uid, iid, rating in trainset.all_ratings():\n",
    "        raw_uid = trainset.to_raw_uid(uid)\n",
    "        raw_iid = trainset.to_raw_iid(iid)\n",
    "        raw_trainset.append((raw_uid, raw_iid, float(rating)))\n",
    "    \n",
    "    raw_testset = [(uid, iid, float(rating)) for uid, iid, rating in testset]\n",
    "    \n",
    "    # Create pandas DataFrames\n",
    "    train_df = pd.DataFrame(raw_trainset, columns=['user_id', 'movie_id', 'rating'])\n",
    "    test_df = pd.DataFrame(raw_testset, columns=['user_id', 'movie_id', 'rating'])\n",
    "    \n",
    "    # Extract unique users and movies\n",
    "    unique_user_ids = sorted(list(set(train_df['user_id'].unique()).union(set(test_df['user_id'].unique()))))\n",
    "    unique_movie_ids = sorted(list(set(train_df['movie_id'].unique()).union(set(test_df['movie_id'].unique()))))\n",
    "    \n",
    "    # Create vocabularies\n",
    "    user_vocab = tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None)\n",
    "    movie_vocab = tf.keras.layers.StringLookup(vocabulary=unique_movie_ids, mask_token=None)\n",
    "    \n",
    "    # Convert to TensorFlow datasets\n",
    "    def df_to_tf_dataset(df, shuffle=True, batch_size=128):\n",
    "        features = {\n",
    "            \"user_id\": tf.cast(df[\"user_id\"].values, tf.string),\n",
    "            \"movie_id\": tf.cast(df[\"movie_id\"].values, tf.string),\n",
    "        }\n",
    "        \n",
    "        # Separate the targets (ratings)\n",
    "        labels = tf.cast(df[\"rating\"].values, tf.float32)\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(df))\n",
    "            \n",
    "        return dataset.batch(batch_size)\n",
    "    \n",
    "    train_dataset = df_to_tf_dataset(train_df)\n",
    "    test_dataset = df_to_tf_dataset(test_df, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, user_vocab, movie_vocab, len(unique_user_ids), len(unique_movie_ids)\n",
    "\n",
    "def build_ncf_model(user_vocab, movie_vocab, num_users, num_movies):\n",
    "    \"\"\"Build Neural Collaborative Filtering model\"\"\"\n",
    "    class NCFModel(tf.keras.Model):\n",
    "        def __init__(self, user_vocab, movie_vocab, num_users, num_movies):\n",
    "            super().__init__()\n",
    "            \n",
    "            # User embedding\n",
    "            self.user_lookup = user_vocab\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users + 1, 32, name=\"user_embedding\")\n",
    "            \n",
    "            # Movie embedding\n",
    "            self.movie_lookup = movie_vocab\n",
    "            self.movie_embedding = tf.keras.layers.Embedding(\n",
    "                num_movies + 1, 32, name=\"movie_embedding\")\n",
    "            \n",
    "            # MLP layers\n",
    "            self.dense_layers = [\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(16, activation='relu'),\n",
    "            ]\n",
    "            \n",
    "            # Output layer\n",
    "            self.rating_pred = tf.keras.layers.Dense(1)\n",
    "            \n",
    "        def call(self, inputs):\n",
    "            # Get user and movie IDs\n",
    "            user_id = inputs[\"user_id\"]\n",
    "            movie_id = inputs[\"movie_id\"]\n",
    "            \n",
    "            # Look up embeddings\n",
    "            user_embed = self.user_embedding(self.user_lookup(user_id))\n",
    "            movie_embed = self.movie_embedding(self.movie_lookup(movie_id))\n",
    "            \n",
    "            # Concatenate embeddings\n",
    "            concat = tf.concat([user_embed, movie_embed], axis=1)\n",
    "            \n",
    "            # Pass through dense layers\n",
    "            x = concat\n",
    "            for layer in self.dense_layers:\n",
    "                x = layer(x)\n",
    "                \n",
    "            # Output prediction\n",
    "            return self.rating_pred(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = NCFModel(user_vocab, movie_vocab, num_users, num_movies)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_keras_model(model, name):\n",
    "    \"\"\"Save a Keras model properly using TF SavedModel format\"\"\"\n",
    "    # Make sure the models directory exists\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    # Path for the saved model\n",
    "    model_path = os.path.join('models', name)\n",
    "    \n",
    "    try:\n",
    "        # Save the model using TensorFlow SavedModel format\n",
    "        # This works for subclassed models\n",
    "        model.save(model_path, save_format=\"tf\")\n",
    "        print(f\"Model saved successfully to {model_path}\")\n",
    "        \n",
    "        # Save model info\n",
    "        model_info = {\n",
    "            'model_type': 'Neural Collaborative Filtering',\n",
    "            'model_path': model_path,\n",
    "            'is_tensorflow': True,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        info_path = os.path.join('models', 'ncf_model_info.pkl')\n",
    "        with open(info_path, 'wb') as f:\n",
    "            pickle.dump(model_info, f)\n",
    "        print(f\"Model info saved to {info_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "        \n",
    "        # Try saving weights as a fallback\n",
    "        try:\n",
    "            weights_path = os.path.join('models', f\"{name}_weights.h5\")\n",
    "            model.save_weights(weights_path)\n",
    "            print(f\"Saved model weights to {weights_path}\")\n",
    "            \n",
    "            # Update the info file\n",
    "            model_info = {\n",
    "                'model_type': 'Neural Collaborative Filtering',\n",
    "                'weights_path': weights_path,\n",
    "                'is_weights_only': True,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            info_path = os.path.join('models', 'ncf_model_info.pkl')\n",
    "            with open(info_path, 'wb') as f:\n",
    "                pickle.dump(model_info, f)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"Error saving weights: {e2}\")\n",
    "            return False\n",
    "\n",
    "def train_and_save_ncf():\n",
    "    \"\"\"Train and save the Neural Collaborative Filtering model\"\"\"\n",
    "    print(\"Starting Neural Collaborative Filtering model training...\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_dataset, test_dataset, user_vocab, movie_vocab, num_users, num_movies = load_movielens_data()\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_ncf_model(user_vocab, movie_vocab, num_users, num_movies)\n",
    "    \n",
    "    # Get a sample batch from the dataset to build the model\n",
    "    for features_batch, _ in train_dataset.take(1):\n",
    "        # This will build the model by running a forward pass\n",
    "        _ = model(features_batch)\n",
    "        break\n",
    "    \n",
    "    # Now we can call summary\n",
    "    print(\"Model built successfully. Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nTraining the model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=20,  # Reduced epochs for quicker training\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    evaluation = model.evaluate(test_dataset, return_dict=True)\n",
    "    rmse = np.sqrt(evaluation['loss'])\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving the model...\")\n",
    "    if save_keras_model(model, \"ncf_model\"):\n",
    "        print(\"Neural Collaborative Filtering model saved successfully\")\n",
    "    else:\n",
    "        print(\"Failed to save the model\")\n",
    "    \n",
    "    print(\"\\nModel training and saving complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_save_ncf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
